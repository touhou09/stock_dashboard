"""
Bronze Layer - Delta Lake ê¸°ë°˜ ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
GCSì— Delta Table í˜•íƒœë¡œ S&P 500 ë°ì´í„°ì™€ ë°°ë‹¹ ë°ì´í„°ë¥¼ ì €ì¥
"""

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
import time
import random
import requests
from io import StringIO
from typing import List, Tuple, Optional
import os
import logging
from deltalake import DeltaTable, write_deltalake
import pyarrow as pa
from google.cloud import storage

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

WIKI_URL = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"

class BronzeLayerDelta:
    """Bronze Layer Delta Lake ê¸°ë°˜ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, gcs_bucket: str, gcs_path: str = "stock_dashboard/bronze"):
        """
        Bronze Layer ì´ˆê¸°í™”
        
        Args:
            gcs_bucket: GCS ë²„í‚· ì´ë¦„
            gcs_path: GCS ë‚´ ê²½ë¡œ
        """
        self.gcs_bucket = gcs_bucket
        self.gcs_path = gcs_path
        self.storage_client = storage.Client()
        self.bucket = self.storage_client.bucket(gcs_bucket)
        
        # Delta Table ê²½ë¡œ ì„¤ì •
        self.price_table_path = f"gs://{gcs_bucket}/{gcs_path}/sp500_daily_prices"
        self.dividend_table_path = f"gs://{gcs_bucket}/{gcs_path}/sp500_dividend_info"
    
    def to_yahoo_symbol(self, sym: str) -> str:
        """í´ë˜ìŠ¤ ì£¼ì‹ í‘œê¸°: BRK.B -> BRK-B"""
        return sym.strip().upper().replace(".", "-")
    
    def get_sp500_from_wikipedia(self, max_retries: int = 3, timeout: int = 15) -> pd.DataFrame:
        """Wikipediaì—ì„œ S&P500 êµ¬ì„±ì¢…ëª© í…Œì´ë¸” íŒŒì‹±"""
        headers_pool = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/121.0",
        ]
        last_err = None

        for i in range(max_retries):
            try:
                logger.info(f"Wikipedia ì ‘ê·¼ ì‹œë„ {i+1}/{max_retries}...")
                headers = {
                    "User-Agent": random.choice(headers_pool),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.7",
                    "Cache-Control": "no-cache",
                    "Pragma": "no-cache",
                }
                resp = requests.get(WIKI_URL, headers=headers, timeout=timeout)
                resp.raise_for_status()
                
                tables = pd.read_html(StringIO(resp.text))
                spx = tables[0]
                
                if "Symbol" not in spx.columns:
                    candidates = [c for c in spx.columns if "symbol" in c.lower() or "ticker" in c.lower()]
                    if candidates:
                        spx = spx.rename(columns={candidates[0]: "Symbol"})
                    else:
                        raise ValueError("Symbol ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                logger.info(f"âœ… Wikipediaì—ì„œ S&P 500 ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {len(spx)}ê°œ ì¢…ëª©")
                return spx
                
            except Exception as e:
                last_err = e
                logger.error(f"âŒ Wikipedia ì ‘ê·¼ ì‹¤íŒ¨ (ì‹œë„ {i+1}): {e}")
                if i < max_retries - 1:
                    wait_time = 1.5 * (i + 1)
                    logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
        
        raise RuntimeError(f"Wikipedia íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {last_err}")
    
    def normalize_symbols(self, df: pd.DataFrame) -> pd.DataFrame:
        """Yahoo í˜•ì‹ìœ¼ë¡œ ì‹¬ë³¼ ì •ê·œí™”"""
        df = df.copy()
        df["Symbol"] = df["Symbol"].astype(str).map(self.to_yahoo_symbol)
        return df
    
    def get_daily_data_for_tickers(self, tickers: List[str], target_date: datetime.date) -> Tuple[List[pd.DataFrame], List[str], List[str]]:
        """ì „ì²´ S&P 500ì˜ í•˜ë£¨ì¹˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        logger.info(f" {target_date} í•˜ë£¨ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f" ì „ì²´ ì¢…ëª© ìˆ˜: {len(tickers)}ê°œ")
        
        all_daily_data = []
        successful_tickers = []
        failed_tickers = []
        
        for i, ticker in enumerate(tickers):
            logger.info(f"  ì²˜ë¦¬ ì¤‘: {ticker} ({i+1}/{len(tickers)})")
            
            try:
                stock = yf.Ticker(ticker)
                
                # í•˜ë£¨ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                start_date = target_date
                end_date = target_date + timedelta(days=1)
                
                hist = stock.history(start=start_date, end=end_date)
                
                if not hist.empty and hist['Close'].notna().any():
                    # ë°ì´í„° ì²˜ë¦¬
                    hist['ticker'] = ticker
                    hist['date'] = hist.index
                    hist = hist.reset_index(drop=True)
                    all_daily_data.append(hist)
                    successful_tickers.append(ticker)
                    
                    logger.info(f"    âœ… {ticker}: ${hist['Close'].iloc[-1]:.2f}")
                else:
                    failed_tickers.append(ticker)
                    logger.info(f"    âŒ {ticker}: ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                failed_tickers.append(ticker)
                logger.error(f"    âŒ {ticker}: {e}")
            
            # API ì œí•œ ê³ ë ¤í•œ ë”œë ˆì´
            time.sleep(0.5)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ (50ê°œë§ˆë‹¤)
            if (i + 1) % 50 == 0:
                logger.info(f"    ğŸ“Š ì§„í–‰ë¥ : {i+1}/{len(tickers)} ({((i+1)/len(tickers)*100):.1f}%)")
                logger.info(f"    âœ… ì„±ê³µ: {len(successful_tickers)}ê°œ, âŒ ì‹¤íŒ¨: {len(failed_tickers)}ê°œ")
        
        logger.info(f"\nğŸ“ˆ ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:")
        logger.info(f"  âœ… ì„±ê³µ: {len(successful_tickers)}ê°œ")
        logger.info(f"  âŒ ì‹¤íŒ¨: {len(failed_tickers)}ê°œ")
        logger.info(f"  ë°ì´í„° í¬ì¸íŠ¸: {sum(len(df) for df in all_daily_data)}ê°œ")
        
        return all_daily_data, successful_tickers, failed_tickers
    
    def get_dividend_info_for_tickers(self, tickers: List[str], sample_size: Optional[int] = None) -> List[dict]:
        """ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘"""
        if sample_size is None:
            sample_size = len(tickers)
        
        logger.info(f"\nğŸ’° ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘ ì¤‘... (ìƒìœ„ {min(sample_size, len(tickers))}ê°œ ì¢…ëª©)")
        
        dividend_info = []
        successful_count = 0
        
        for i, ticker in enumerate(tickers[:sample_size]):
            logger.info(f"  ì²˜ë¦¬ ì¤‘: {ticker} ({i+1}/{min(sample_size, len(tickers))})")
            
            try:
                stock = yf.Ticker(ticker)
                info = stock.info
                
                # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
                dividend_yield = info.get('dividendYield', 0) or 0
                dividend_rate = info.get('dividendRate', 0) or 0
                ex_dividend_date = info.get('exDividendDate', None)
                payment_date = info.get('dividendDate', None)
                dividend_frequency = info.get('dividendFrequency', None)
                
                # ë°°ë‹¹ì£¼ ì—¬ë¶€ íŒë‹¨
                has_dividend = dividend_yield > 0 or dividend_rate > 0
                
                dividend_info.append({
                    'ticker': ticker,
                    'company_name': info.get('longName', 'N/A'),
                    'sector': info.get('sector', 'N/A'),
                    'has_dividend': has_dividend,
                    'dividend_yield': dividend_yield,
                    'dividend_yield_percent': dividend_yield * 100 if dividend_yield else 0,
                    'dividend_rate': dividend_rate,
                    'ex_dividend_date': ex_dividend_date,
                    'payment_date': payment_date,
                    'dividend_frequency': dividend_frequency,
                    'market_cap': info.get('marketCap', 0),
                    'last_price': info.get('currentPrice', 0)
                })
                
                successful_count += 1
                logger.info(f"    âœ… {info.get('longName', 'N/A')[:30]}")
                if has_dividend:
                    logger.info(f"    ğŸ’° ë°°ë‹¹ìˆ˜ìµë¥ : {dividend_yield:.2%}")
                
            except Exception as e:
                logger.error(f"    âŒ {ticker}: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì¶”ê°€
                dividend_info.append({
                    'ticker': ticker,
                    'company_name': 'N/A',
                    'sector': 'N/A',
                    'has_dividend': False,
                    'dividend_yield': 0,
                    'dividend_yield_percent': 0,
                    'dividend_rate': 0,
                    'ex_dividend_date': None,
                    'payment_date': None,
                    'dividend_frequency': None,
                    'market_cap': 0,
                    'last_price': 0
                })
            
            # API ì œí•œ ê³ ë ¤
            time.sleep(0.3)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ (50ê°œë§ˆë‹¤)
            if (i + 1) % 50 == 0:
                logger.info(f"    ğŸ“Š ì§„í–‰ë¥ : {i+1}/{min(sample_size, len(tickers))} ({((i+1)/min(sample_size, len(tickers))*100):.1f}%)")
                logger.info(f"    âœ… ì„±ê³µ: {successful_count}ê°œ")
        
        return dividend_info
    
    def save_price_data_to_delta(self, all_daily_data: List[pd.DataFrame], target_date: datetime.date):
        """ê°€ê²© ë°ì´í„°ë¥¼ Delta Tableì— ì €ì¥"""
        logger.info(f"\nï¿½ï¿½ ê°€ê²© ë°ì´í„°ë¥¼ Delta Tableì— ì €ì¥ ì¤‘...")
        
        if not all_daily_data:
            logger.warning("ì €ì¥í•  ê°€ê²© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # pandas DataFrame ê²°í•©
        combined_df = pd.concat(all_daily_data, ignore_index=True)
        
        # ë°ì´í„° ì •ë¦¬
        combined_df['date'] = pd.to_datetime(combined_df['date']).dt.date
        combined_df['ingestion_timestamp'] = datetime.now()
        
        try:
            # Delta Tableì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            delta_table = DeltaTable(self.price_table_path)
            mode = "append"
            logger.info("âœ… ê¸°ì¡´ Delta Tableì— ë°ì´í„° ì¶”ê°€")
        except Exception:
            mode = "overwrite"
            logger.info("ğŸ†• ìƒˆë¡œìš´ Delta Table ìƒì„±")
        
        # Delta Tableì— ì €ì¥
        write_deltalake(
            self.price_table_path,
            combined_df,
            mode=mode,
            partition_by=["date"],  # ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹
            engine="pyarrow"
        )
        
        logger.info(f"âœ… ê°€ê²© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(combined_df)}í–‰")
        logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.price_table_path}")
    
    def save_dividend_data_to_delta(self, dividend_info: List[dict], target_date: datetime.date):
        """ë°°ë‹¹ ì •ë³´ë¥¼ Delta Tableì— ì €ì¥"""
        logger.info(f"\nğŸ’¾ ë°°ë‹¹ ì •ë³´ë¥¼ Delta Tableì— ì €ì¥ ì¤‘...")
        
        if not dividend_info:
            logger.warning("ì €ì¥í•  ë°°ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # pandas DataFrameìœ¼ë¡œ ë³€í™˜
        dividend_df = pd.DataFrame(dividend_info)
        dividend_df['ingestion_timestamp'] = datetime.now()
        
        try:
            # Delta Tableì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            delta_table = DeltaTable(self.dividend_table_path)
            mode = "append"
            logger.info("âœ… ê¸°ì¡´ Delta Tableì— ë°ì´í„° ì¶”ê°€")
        except Exception:
            mode = "overwrite"
            logger.info("ğŸ†• ìƒˆë¡œìš´ Delta Table ìƒì„±")
        
        # Delta Tableì— ì €ì¥
        write_deltalake(
            self.dividend_table_path,
            dividend_df,
            mode=mode,
            partition_by=["has_dividend"],  # ë°°ë‹¹ì£¼ ì—¬ë¶€ë³„ íŒŒí‹°ì…”ë‹
            engine="pyarrow"
        )
        
        logger.info(f"âœ… ë°°ë‹¹ ì •ë³´ ì €ì¥ ì™„ë£Œ: {len(dividend_df)}í–‰")
        logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.dividend_table_path}")
    
    def run_daily_collection(self, target_date: Optional[datetime.date] = None):
        """ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        if target_date is None:
            target_date = datetime.now().date() - timedelta(days=1)
        
        logger.info("=" * 80)
        logger.info(" S&P 500 Bronze Layer ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ (Delta Lake)")
        logger.info("=" * 80)
        logger.info(f" ìˆ˜ì§‘ ë‚ ì§œ: {target_date}")
        
        try:
            # 1. S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            logger.info("\n1ï¸âƒ£ S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘...")
            spx_raw = self.get_sp500_from_wikipedia()
            spx = self.normalize_symbols(spx_raw)
            tickers = spx["Symbol"].dropna().unique().tolist()
            logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(tickers)}ê°œ ì¢…ëª©")
            
            # 2. ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            logger.info(f"\n2ï¸âƒ£ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘...")
            all_daily_data, successful_tickers, failed_tickers = self.get_daily_data_for_tickers(tickers, target_date)
            
            if all_daily_data:
                self.save_price_data_to_delta(all_daily_data, target_date)
            else:
                logger.error("âŒ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return
            
            # 3. ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘
            logger.info(f"\n3ï¸âƒ£ ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘...")
            dividend_info = self.get_dividend_info_for_tickers(successful_tickers, sample_size=200)
            self.save_dividend_data_to_delta(dividend_info, target_date)
            
            # 4. ìµœì¢… ìš”ì•½
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“ˆ Bronze Layer ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 80)
            logger.info(f" ìˆ˜ì§‘ ë‚ ì§œ: {target_date}")
            logger.info(f" ì „ì²´ ì¢…ëª© ìˆ˜: {len(tickers)}ê°œ")
            logger.info(f"âœ… ì„±ê³µí•œ ì¢…ëª©: {len(successful_tickers)}ê°œ")
            logger.info(f"âŒ ì‹¤íŒ¨í•œ ì¢…ëª©: {len(failed_tickers)}ê°œ")
            logger.info(f"ğŸ’° ë°°ë‹¹ì£¼ ì¢…ëª©: {len([d for d in dividend_info if d['has_dividend']])}ê°œ")
            logger.info(f" ì €ì¥ëœ Delta Table:")
            logger.info(f"  - {self.price_table_path}")
            logger.info(f"  - {self.dividend_table_path}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"âŒ Bronze Layer ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # GCS ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    gcs_bucket = os.getenv("GCS_BUCKET", "your-stock-dashboard-bucket")
    
    bronze_layer = BronzeLayerDelta(gcs_bucket=gcs_bucket)
    
    try:
        bronze_layer.run_daily_collection()
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()
