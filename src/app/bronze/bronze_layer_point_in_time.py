"""
Point-in-Time Bronze Layer - í¸ì…ì¼ ê¸°ì¤€ ë°±í•„ ì§€ì›
ìƒì¡´ í¸í–¥ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì‹œì ë³„ ì •í™•í•œ ë°ì´í„° ìˆ˜ì§‘
"""

import pandas as pd
import yfinance as yf
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Optional, Tuple
import logging
from deltalake import DeltaTable, write_deltalake
import pyarrow as pa
from dotenv import load_dotenv
import time

from src.app.membership.sp500_membership_tracker import SP500MembershipTracker
from src.utils.data_storage import DeltaStorageManager

try:
    load_dotenv()
except Exception:
    pass
logger = logging.getLogger(__name__)

class BronzeLayerPointInTime:
    """Point-in-Time Bronze Layer - í¸ì…ì¼ ê¸°ì¤€ ë°±í•„ ì§€ì›"""
    
    def __init__(self, gcs_bucket: str, gcs_path: str = "stock_dashboard/bronze"):
        """
        ì´ˆê¸°í™”
        
        Args:
            gcs_bucket: GCS ë²„í‚· ì´ë¦„
            gcs_path: GCS ê²½ë¡œ
        """
        self.gcs_bucket = gcs_bucket
        self.gcs_path = gcs_path
        
        # ë©¤ë²„ì‹­ ì¶”ì ê¸° ì´ˆê¸°í™”
        self.membership_tracker = SP500MembershipTracker(gcs_bucket)
        
        # Delta Storage Manager ì´ˆê¸°í™”
        self.storage_manager = DeltaStorageManager(gcs_bucket, gcs_path)
    
    def get_constituents_for_date(self, target_date: date) -> List[str]:
        """
        íŠ¹ì • ë‚ ì§œì˜ S&P 500 êµ¬ì„± ì¢…ëª© ì¡°íšŒ
        
        Args:
            target_date: ëŒ€ìƒ ë‚ ì§œ
            
        Returns:
            List[str]: í•´ë‹¹ ë‚ ì§œì˜ êµ¬ì„± ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ“‹ {target_date} ë‚ ì§œì˜ S&P 500 êµ¬ì„± ì¢…ëª© ì¡°íšŒ ì¤‘...")
        
        try:
            # ì¼ìë³„ ë©¤ë²„ì‹­ ì¡°íšŒ
            daily_membership = self.membership_tracker.get_daily_membership(target_date)
            
            if not daily_membership.empty:
                # í•´ë‹¹ ë‚ ì§œì— ë©¤ë²„ì¸ ì¢…ëª©ë“¤ë§Œ í•„í„°ë§
                members = daily_membership[daily_membership['is_member'] == True]
                tickers = members['ticker'].unique().tolist()
                
                logger.info(f"âœ… {target_date} êµ¬ì„± ì¢…ëª©: {len(tickers)}ê°œ")
                return tickers
            else:
                logger.warning(f"âš ï¸ {target_date} ë©¤ë²„ì‹­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì—°ë„ êµ¬ì„±ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                # ë©¤ë²„ì‹­ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° í•´ë‹¹ ì—°ë„ êµ¬ì„±ìœ¼ë¡œ ëŒ€ì²´
                target_year = target_date.year
                current_sp500 = self.membership_tracker.get_sp500_for_year(target_year)
                return current_sp500['Symbol'].tolist()
                
        except Exception as e:
            logger.error(f"âŒ {target_date} êµ¬ì„± ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œ í•´ë‹¹ ì—°ë„ êµ¬ì„±ìœ¼ë¡œ ëŒ€ì²´
            target_year = target_date.year
            current_sp500 = self.membership_tracker.get_sp500_for_year(target_year)
            return current_sp500['Symbol'].tolist()
    
    def get_price_data_for_date(self, tickers: List[str], target_date: date, batch_size: int = 50) -> Tuple[List[pd.DataFrame], List[str], List[str]]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
        
        Args:
            tickers: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
            target_date: ëŒ€ìƒ ë‚ ì§œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            Tuple[List[pd.DataFrame], List[str], List[str]]: (ë°ì´í„°, ì„±ê³µì¢…ëª©, ì‹¤íŒ¨ì¢…ëª©)
        """
        logger.info(f"ğŸ“Š {target_date} ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ì´ {len(tickers)}ê°œ ì¢…ëª©)")
        
        all_data = []
        successful = []
        failed = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for batch_num in range(0, len(tickers), batch_size):
            batch_tickers = tickers[batch_num:batch_num + batch_size]
            batch_idx = batch_num // batch_size + 1
            total_batches = (len(tickers) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ”„ ë°°ì¹˜ {batch_idx}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_tickers)}ê°œ ì¢…ëª©)")
            
            batch_data, batch_successful, batch_failed = self._collect_batch_price_data(batch_tickers, target_date)
            
            all_data.extend(batch_data)
            successful.extend(batch_successful)
            failed.extend(batch_failed)
            
            # API ì œí•œ ë°©ì§€
            time.sleep(0.5)
        
        logger.info(f"âœ… {target_date} ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ì„±ê³µ {len(successful)}ê°œ, ì‹¤íŒ¨ {len(failed)}ê°œ")
        return all_data, successful, failed
    
    def _collect_batch_price_data(self, batch_tickers: List[str], target_date: date) -> Tuple[List[pd.DataFrame], List[str], List[str]]:
        """ë°°ì¹˜ ë‹¨ìœ„ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘"""
        batch_data = []
        batch_successful = []
        batch_failed = []
        
        for ticker in batch_tickers:
            try:
                yf_ticker = yf.Ticker(ticker)
                hist = yf_ticker.history(start=target_date, end=target_date + timedelta(days=1))
                
                if not hist.empty:
                    hist_df = hist.reset_index()
                    
                    # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦ ë° ì¶”ê°€
                    required_columns = ['open', 'high', 'low', 'close', 'volume', 'adj_close']
                    for col in required_columns:
                        if col not in hist_df.columns:
                            if col == 'adj_close':
                                hist_df[col] = hist_df.get('Close', hist_df.get('close', 0))  # ì¡°ì •ì£¼ê°€ ì—†ìœ¼ë©´ ì¢…ê°€ ì‚¬ìš©
                            else:
                                hist_df[col] = 0  # ê¸°ë³¸ê°’ ì„¤ì •
                    
                    hist_df['ticker'] = ticker
                    hist_df['date'] = target_date
                    hist_df['ingest_at'] = datetime.now()  # ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì— ë§ì¶° ë³µì›
                    
                    batch_data.append(hist_df)
                    batch_successful.append(ticker)
                    logger.debug(f"âœ… {ticker} ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ")
                else:
                    batch_failed.append(ticker)
                    logger.warning(f"âš ï¸ {ticker} ê°€ê²© ë°ì´í„° ì—†ìŒ")
                
                # API ì œí•œ ë°©ì§€
                time.sleep(0.1)
                
            except Exception as e:
                batch_failed.append(ticker)
                logger.error(f"âŒ {ticker} ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return batch_data, batch_successful, batch_failed
    
    def get_dividend_data_for_date(self, tickers: List[str], target_date: date, lookback_days: int = 400) -> pd.DataFrame:
        """
        íŠ¹ì • ë‚ ì§œì˜ ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            tickers: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
            target_date: ëŒ€ìƒ ë‚ ì§œ
            lookback_days: ë°°ë‹¹ ì´ë ¥ ì¡°íšŒ ì¼ìˆ˜
            
        Returns:
            pd.DataFrame: ë°°ë‹¹ ì´ë²¤íŠ¸ ë°ì´í„°
        """
        logger.info(f"ğŸ’° {target_date} ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (TTM: {lookback_days}ì¼)")
        
        dividend_events_list = []
        since_date = target_date - timedelta(days=lookback_days)
        
        for i, ticker in enumerate(tickers):
            try:
                logger.info(f"ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {ticker} ({i+1}/{len(tickers)})")
                
                yf_ticker = yf.Ticker(ticker)
                
                # ë°°ë‹¹ ì´ë ¥ ì¡°íšŒ
                dividend_history = yf_ticker.dividends
                
                if not dividend_history.empty:
                    # ê¸°ê°„ í•„í„°ë§
                    dividend_history = dividend_history[
                        (dividend_history.index.date >= since_date) & 
                        (dividend_history.index.date <= target_date)
                    ]
                    
                    # ë°°ë‹¹ ì´ë²¤íŠ¸ë¡œ ë³€í™˜
                    for ex_date, amount in dividend_history.items():
                        dividend_events_list.append({
                            'ex_date': ex_date.date(),
                            'ticker': ticker,
                            'amount': amount,
                            'date': target_date,  # ìˆ˜ì§‘ ë‚ ì§œ
                            'ingest_at': datetime.now()
                        })
                
                # API ì œí•œ ë°©ì§€
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"âŒ {ticker} ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        dividend_df = pd.DataFrame(dividend_events_list)
        logger.info(f"âœ… {target_date} ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(dividend_df)}ê°œ ì´ë²¤íŠ¸")
        
        return dividend_df
    
    def run_point_in_time_collection(self, target_date: date, batch_size: int = 50) -> bool:
        """
        Point-in-Time ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        
        Args:
            target_date: ëŒ€ìƒ ë‚ ì§œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        logger.info("=" * 80)
        logger.info("ğŸ“Š Point-in-Time Bronze Layer ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 80)
        logger.info(f" ìˆ˜ì§‘ ë‚ ì§œ: {target_date}")
        logger.info(f" ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œì”© ì²˜ë¦¬")
        
        try:
            # 1. í•´ë‹¹ ë‚ ì§œì˜ S&P 500 êµ¬ì„± ì¢…ëª© ì¡°íšŒ
            logger.info(f"\n1ï¸âƒ£ {target_date} S&P 500 êµ¬ì„± ì¢…ëª© ì¡°íšŒ...")
            tickers = self.get_constituents_for_date(target_date)
            
            if not tickers:
                logger.error("âŒ êµ¬ì„± ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 2. ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
            logger.info(f"\n2ï¸âƒ£ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘...")
            price_data, successful_tickers, failed_tickers = self.get_price_data_for_date(tickers, target_date, batch_size)
            
            if price_data:
                # ê°€ê²© ë°ì´í„° ì €ì¥
                self.storage_manager.save_price_data_to_delta(price_data, target_date)
                logger.info(f"âœ… ê°€ê²© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(price_data)}ê°œ")
            
            # 3. ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘ (ì „ì²´ ì¢…ëª© ëŒ€ìƒ)
            logger.info(f"\n3ï¸âƒ£ ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘...")
            dividend_df = self.get_dividend_data_for_date(tickers, target_date)
            
            if not dividend_df.empty:
                # ë°°ë‹¹ ë°ì´í„° ì €ì¥
                self.storage_manager.save_dividend_events_to_delta(dividend_df)
                logger.info(f"âœ… ë°°ë‹¹ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(dividend_df)}ê°œ ì´ë²¤íŠ¸")
            
            # 4. ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“ˆ Point-in-Time ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 80)
            logger.info(f" ìˆ˜ì§‘ ë‚ ì§œ: {target_date}")
            logger.info(f"ğŸ“Š êµ¬ì„± ì¢…ëª© ìˆ˜: {len(tickers)}ê°œ")
            logger.info(f"ğŸ“Š ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {len(successful_tickers)}ê°œ")
            logger.info(f"ğŸ“Š ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {len(failed_tickers)}ê°œ")
            logger.info(f"ğŸ“Š ë°°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ì§‘: {len(dividend_df)}ê°œ")
            logger.info("=" * 80)
            
            # ì„±ê³µë¥ ì´ 90% ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (ì¼ë¶€ ì¢…ëª© ì‹¤íŒ¨ í—ˆìš©)
            success_rate = len(successful_tickers) / (len(successful_tickers) + len(failed_tickers)) if (len(successful_tickers) + len(failed_tickers)) > 0 else 0
            is_success = success_rate >= 0.9  # 90% ì´ìƒ ì„±ê³µë¥ 
            
            if is_success:
                logger.info(f"âœ… {target_date} ì²˜ë¦¬ ì„±ê³µ (ì„±ê³µë¥ : {success_rate:.1%})")
            else:
                logger.warning(f"âš ï¸ {target_date} ì²˜ë¦¬ ë¶€ë¶„ ì„±ê³µ (ì„±ê³µë¥ : {success_rate:.1%})")
            
            return is_success
            
        except Exception as e:
            logger.error(f"âŒ Point-in-Time ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return False
    
    def run_point_in_time_backfill(self, start_date: date, end_date: date, batch_size: int = 50) -> bool:
        """
        Point-in-Time ë°±í•„ ì‹¤í–‰
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        logger.info("=" * 80)
        logger.info("ğŸ”„ Point-in-Time Bronze Layer ë°±í•„ ì‹œì‘")
        logger.info("=" * 80)
        logger.info(f" ë°±í•„ ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info(f" ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œì”© ì²˜ë¦¬")
        
        try:
            # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (í‰ì¼ë§Œ)
            date_list = []
            current_date = start_date
            while current_date <= end_date:
                if current_date.weekday() < 5:  # 0-4: ì›”-ê¸ˆ
                    date_list.append(current_date)
                current_date += timedelta(days=1)
            
            total_dates = len(date_list)
            logger.info(f"ğŸ“Š ë°±í•„í•  ë‚ ì§œ ìˆ˜: {total_dates}ê°œ (í‰ì¼ë§Œ)")
            
            if total_dates == 0:
                logger.info("âœ… ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            successful_dates = []
            failed_dates = []
            
            # ê° ë‚ ì§œë³„ë¡œ Point-in-Time ìˆ˜ì§‘
            for i, target_date in enumerate(date_list, 1):
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“… Point-in-Time {i}/{total_dates} ì²˜ë¦¬ ì¤‘: {target_date}")
                logger.info(f"{'='*60}")
                
                try:
                    success = self.run_point_in_time_collection(target_date, batch_size)
                    
                    if success:
                        successful_dates.append(target_date)
                        logger.info(f"âœ… {target_date} Point-in-Time ì²˜ë¦¬ ì™„ë£Œ")
                    else:
                        failed_dates.append((target_date, "Point-in-Time ìˆ˜ì§‘ ì‹¤íŒ¨"))
                        logger.error(f"âŒ {target_date} Point-in-Time ì²˜ë¦¬ ì‹¤íŒ¨")
                        
                except Exception as e:
                    failed_dates.append((target_date, str(e)))
                    logger.error(f"âŒ {target_date} Point-in-Time ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            # ë°±í•„ ê²°ê³¼ ìš”ì•½
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“ˆ Point-in-Time ë°±í•„ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 80)
            logger.info(f" ì „ì²´ ì²˜ë¦¬ ë‚ ì§œ: {total_dates}ê°œ")
            logger.info(f" ì„±ê³µí•œ ë‚ ì§œ: {len(successful_dates)}ê°œ")
            logger.info(f" ì‹¤íŒ¨í•œ ë‚ ì§œ: {len(failed_dates)}ê°œ")
            
            if failed_dates:
                logger.info(f"\nâŒ ì‹¤íŒ¨í•œ ë‚ ì§œ:")
                for date, error in failed_dates[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    logger.info(f"   - {date}: {error}")
                if len(failed_dates) > 10:
                    logger.info(f"   ... ì™¸ {len(failed_dates) - 10}ê°œ")
            
            logger.info("=" * 80)
            return len(failed_dates) == 0
            
        except Exception as e:
            logger.error(f"âŒ Point-in-Time ë°±í•„ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Point-in-Time Bronze Layer")
    parser.add_argument("--mode", choices=["single", "backfill"], default="single", help="ì‹¤í–‰ ëª¨ë“œ")
    parser.add_argument("--date", type=str, help="ì²˜ë¦¬ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--start-date", type=str, help="ë°±í•„ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="ë°±í•„ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--batch-size", type=int, default=50, help="ë°°ì¹˜ í¬ê¸°")
    
    args = parser.parse_args()
    
    # GCS ì„¤ì •
    gcs_bucket = os.getenv("GCS_BUCKET", "your-stock-dashboard-bucket")
    bronze_pit = BronzeLayerPointInTime(gcs_bucket)
    
    try:
        if args.mode == "single":
            if not args.date:
                print("âŒ single ëª¨ë“œì—ì„œëŠ” --dateê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return 1
            
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
            success = bronze_pit.run_point_in_time_collection(target_date, args.batch_size)
            
        elif args.mode == "backfill":
            if not args.start_date or not args.end_date:
                print("âŒ backfill ëª¨ë“œì—ì„œëŠ” --start-dateì™€ --end-dateê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return 1
            
            start_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
            end_date = datetime.strptime(args.end_date, "%Y-%m-%d").date()
            success = bronze_pit.run_point_in_time_backfill(start_date, end_date, args.batch_size)
        
        if success:
            print("ğŸ‰ Point-in-Time Bronze Layer ì™„ë£Œ!")
            return 0
        else:
            print("âŒ Point-in-Time Bronze Layer ì‹¤íŒ¨!")
            return 1
            
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
