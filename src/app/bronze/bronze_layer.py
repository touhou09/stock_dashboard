"""
Bronze Layer ë©”ì¸ í´ë˜ìŠ¤
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¡°ìœ¨
- Backfill ê¸°ëŠ¥
"""

import os
from datetime import datetime, timedelta
from typing import Optional, List
import logging
from dotenv import load_dotenv

# ëª¨ë“ˆ import
from ...utils.data_collectors import SP500Collector, PriceDataCollector, DividendDataCollector
from ...utils.data_storage import DeltaStorageManager
from ...utils.data_validators import DataValidator, BackfillValidator

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BronzeLayer:
    """Bronze Layer ë©”ì¸ í´ë˜ìŠ¤ - ì›ì²œ ë°ì´í„°ë§Œ ì €ì¥"""
    
    def __init__(self, gcs_bucket: str, gcs_path: str = "stock_dashboard/bronze"):
        """
        Bronze Layer ì´ˆê¸°í™”
        
        Args:
            gcs_bucket: GCS ë²„í‚· ì´ë¦„
            gcs_path: GCS ë‚´ ê²½ë¡œ
        """
        self.storage_manager = DeltaStorageManager(gcs_bucket, gcs_path)
        self.sp500_collector = SP500Collector()
        self.price_collector = PriceDataCollector()
        self.dividend_collector = DividendDataCollector()
        self.data_validator = DataValidator()
        self.backfill_validator = BackfillValidator(self.storage_manager)
    
    def run_daily_collection(self, target_date: Optional[datetime.date] = None):
        """ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ (Bronze - ì›ì²œ ë°ì´í„°ë§Œ)"""
        if target_date is None:
            target_date = datetime.now().date() - timedelta(days=1)
        
        logger.info("=" * 80)
        logger.info("ğŸ“Š S&P 500 Bronze Layer ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ (ì›ì²œ ë°ì´í„°)")
        logger.info("=" * 80)
        logger.info(f" ìˆ˜ì§‘ ë‚ ì§œ: {target_date}")
        
        try:
            # 1. S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            logger.info("\n1ï¸âƒ£ S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘...")
            spx_raw = self.sp500_collector.get_sp500_from_wikipedia()
            spx = self.sp500_collector.normalize_symbols(spx_raw)
            tickers = spx["Symbol"].dropna().unique().tolist()
            logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(tickers)}ê°œ ì¢…ëª©")
            
            # 2. ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (Bronze)
            logger.info(f"\n2ï¸âƒ£ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (Bronze)...")
            all_daily_data, successful_tickers, failed_tickers = self.price_collector.get_daily_data_for_tickers(tickers, target_date)
            
            if all_daily_data:
                # ë°ì´í„° ê²€ì¦
                for data in all_daily_data:
                    self.data_validator.validate_price_data(data)
                
                self.storage_manager.save_price_data_to_delta(all_daily_data, target_date)
            else:
                logger.error("âŒ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return
            
            # 3. ë°°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ì§‘ (Bronze) - ìµœê·¼ 400ì¼ ë²”ìœ„
            logger.info(f"\n3ï¸âƒ£ ë°°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ì§‘ (Bronze)...")
            since = target_date - timedelta(days=400)
            dividend_events_df = self.dividend_collector.fetch_dividend_events_for_tickers(successful_tickers, since, target_date)
            
            if not dividend_events_df.empty:
                # ë°ì´í„° ê²€ì¦
                self.data_validator.validate_dividend_data(dividend_events_df)
                self.storage_manager.save_dividend_events_to_delta(dividend_events_df)
            
            # 4. ìµœì¢… ìš”ì•½
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“ˆ Bronze Layer ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 80)
            logger.info(f" ìˆ˜ì§‘ ë‚ ì§œ: {target_date}")
            logger.info(f" ì „ì²´ ì¢…ëª© ìˆ˜: {len(tickers)}ê°œ")
            logger.info(f"âœ… ì„±ê³µí•œ ì¢…ëª©: {len(successful_tickers)}ê°œ")
            logger.info(f"âŒ ì‹¤íŒ¨í•œ ì¢…ëª©: {len(failed_tickers)}ê°œ")
            logger.info(f" ë°°ë‹¹ ì´ë²¤íŠ¸: {len(dividend_events_df)}ê°œ")
            logger.info(f" ì €ì¥ëœ Bronze Delta Table:")
            logger.info(f"  - {self.storage_manager.price_table_path}")
            logger.info(f"  - {self.storage_manager.dividend_events_table_path}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"âŒ Bronze Layer ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise
    
    def run_backfill(self, start_date: datetime.date, end_date: Optional[datetime.date] = None, 
                     batch_size: int = 30, delay_between_batches: int = 60):
        """
        ì§€ì •ëœ ê¸°ê°„ì˜ ëˆ„ë½ëœ ë°ì´í„°ë¥¼ backfillí•©ë‹ˆë‹¤.
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜ê¹Œì§€)
            batch_size: ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  ë‚ ì§œ ìˆ˜
            delay_between_batches: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        if end_date is None:
            end_date = datetime.now().date() - timedelta(days=1)
        
        logger.info("=" * 80)
        logger.info("ğŸ”„ Bronze Layer Backfill ì‹œì‘")
        logger.info("=" * 80)
        logger.info(f" ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info(f" ë°°ì¹˜ í¬ê¸°: {batch_size}ì¼")
        logger.info(f" ë°°ì¹˜ ê°„ ëŒ€ê¸°: {delay_between_batches}ì´ˆ")
        
        # ê°€ì¥ ì´ë¥¸ ëˆ„ë½ëœ ë‚ ì§œ ì°¾ê¸°
        earliest_missing = self.backfill_validator.find_earliest_missing_date(start_date, end_date)
        if earliest_missing is None:
            logger.info("âœ… ì§€ì •ëœ ê¸°ê°„ì— ëˆ„ë½ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì²˜ë¦¬í•  ë‚ ì§œ ëª©ë¡ ìƒì„± (ì˜ì—…ì¼ë§Œ)
        dates_to_process = self.backfill_validator.generate_trading_dates(earliest_missing, end_date)
        
        logger.info(f"ğŸ“… ì²˜ë¦¬í•  ì˜ì—…ì¼ ìˆ˜: {len(dates_to_process)}ì¼")
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        total_batches = (len(dates_to_process) + batch_size - 1) // batch_size
        successful_dates = []
        failed_dates = []
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(dates_to_process))
            batch_dates = dates_to_process[start_idx:end_idx]
            
            logger.info(f"\nğŸ“¦ ë°°ì¹˜ {batch_num + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            logger.info(f" ì²˜ë¦¬í•  ë‚ ì§œ: {batch_dates[0]} ~ {batch_dates[-1]} ({len(batch_dates)}ì¼)")
            
            batch_successful = []
            batch_failed = []
            
            for date in batch_dates:
                try:
                    logger.info(f"\nğŸ“Š {date} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
                    
                    # í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                    if self.storage_manager.check_existing_data(self.storage_manager.price_table_path, date):
                        logger.info(f"â­ï¸ {date} ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë°°ì¹˜ë‹¹ í•œ ë²ˆë§Œ)
                    if batch_successful == [] and batch_failed == []:  # ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ë‚ ì§œ
                        logger.info("ğŸ“‹ S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘...")
                        spx_raw = self.sp500_collector.get_sp500_from_wikipedia()
                        spx = self.sp500_collector.normalize_symbols(spx_raw)
                        tickers = spx["Symbol"].dropna().unique().tolist()
                        logger.info(f"âœ… ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(tickers)}ê°œ")
                    
                    # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
                    all_daily_data, successful_tickers, failed_tickers = self.price_collector.get_daily_data_for_tickers(tickers, date)
                    
                    if all_daily_data:
                        # ë°ì´í„° ê²€ì¦
                        for data in all_daily_data:
                            self.data_validator.validate_price_data(data)
                        
                        self.storage_manager.save_price_data_to_delta(all_daily_data, date)
                        batch_successful.append(date)
                        logger.info(f"âœ… {date} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                    else:
                        batch_failed.append(date)
                        logger.error(f"âŒ {date} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                    
                except Exception as e:
                    batch_failed.append(date)
                    logger.error(f"âŒ {date} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ë‚ ì§œ ê°„ ì§§ì€ ëŒ€ê¸°
                import time
                time.sleep(2)
            
            successful_dates.extend(batch_successful)
            failed_dates.extend(batch_failed)
            
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num + 1} ì™„ë£Œ:")
            logger.info(f"  âœ… ì„±ê³µ: {len(batch_successful)}ì¼")
            logger.info(f"  âŒ ì‹¤íŒ¨: {len(batch_failed)}ì¼")
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
            if batch_num < total_batches - 1:
                logger.info(f"â³ {delay_between_batches}ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ ë°°ì¹˜ ì‹œì‘...")
                import time
                time.sleep(delay_between_batches)
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“ˆ Backfill ì™„ë£Œ ìš”ì•½")
        logger.info("=" * 80)
        logger.info(f" ì²˜ë¦¬ ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info(f" ì „ì²´ ì˜ì—…ì¼: {len(dates_to_process)}ì¼")
        logger.info(f"âœ… ì„±ê³µ: {len(successful_dates)}ì¼")
        logger.info(f"âŒ ì‹¤íŒ¨: {len(failed_dates)}ì¼")
        
        if successful_dates:
            logger.info(f" ì„±ê³µí•œ ë‚ ì§œ: {successful_dates[0]} ~ {successful_dates[-1]}")
        if failed_dates:
            logger.info(f" ì‹¤íŒ¨í•œ ë‚ ì§œ: {failed_dates}")
        
        logger.info("=" * 80)
    
    def run_smart_backfill(self, days_back: int = 365):
        """
        ìŠ¤ë§ˆíŠ¸ backfill: ìµœê·¼ Nì¼ ì¤‘ì—ì„œ ëˆ„ë½ëœ ë°ì´í„°ë§Œ ìˆ˜ì§‘
        
        Args:
            days_back: ëª‡ ì¼ ì „ë¶€í„° í™•ì¸í• ì§€
        """
        end_date = datetime.now().date() - timedelta(days=1)
        start_date = end_date - timedelta(days=days_back)
        
        logger.info(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ Backfill ì‹œì‘ (ìµœê·¼ {days_back}ì¼ í™•ì¸)")
        self.run_backfill(start_date, end_date)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # GCS ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    gcs_bucket = os.getenv("GCS_BUCKET", "your-stock-dashboard-bucket")
    
    bronze_layer = BronzeLayer(gcs_bucket=gcs_bucket)
    
    try:
        # Bronze Layer ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        bronze_layer.run_daily_collection()
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

def main_backfill():
    """Backfill ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Bronze Layer Backfill")
    parser.add_argument("--start-date", type=str, help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--days-back", type=int, default=365, help="ìŠ¤ë§ˆíŠ¸ backfill ì‹œ í™•ì¸í•  ì¼ìˆ˜")
    parser.add_argument("--smart", action="store_true", help="ìŠ¤ë§ˆíŠ¸ backfill ì‚¬ìš©")
    parser.add_argument("--batch-size", type=int, default=30, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--delay", type=int, default=60, help="ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)")
    
    args = parser.parse_args()
    
    # GCS ì„¤ì •
    gcs_bucket = os.getenv("GCS_BUCKET", "your-stock-dashboard-bucket")
    bronze_layer = BronzeLayer(gcs_bucket=gcs_bucket)
    
    try:
        if args.smart:
            # ìŠ¤ë§ˆíŠ¸ backfill
            bronze_layer.run_smart_backfill(args.days_back)
        else:
            # ì¼ë°˜ backfill
            if not args.start_date:
                raise ValueError("ì¼ë°˜ backfillì„ ìœ„í•´ì„œëŠ” --start-dateê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            start_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
            end_date = None
            if args.end_date:
                end_date = datetime.strptime(args.end_date, "%Y-%m-%d").date()
            
            bronze_layer.run_backfill(start_date, end_date, args.batch_size, args.delay)
            
    except Exception as e:
        logger.error(f"âŒ Backfill ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "backfill":
        # backfill ëª¨ë“œ
        sys.argv.pop(1)  # "backfill" ì œê±°
        main_backfill()
    else:
        # ì¼ë°˜ ëª¨ë“œ
        main()
