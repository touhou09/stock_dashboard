"""
S&P 500 í¸ì…/í‡´ì¶œ ì´ë ¥ ì¶”ì  ì‹œìŠ¤í…œ
ìƒì¡´ í¸í–¥ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì‹œì ë³„ ì •í™•í•œ êµ¬ì„± ì¢…ëª© ì¶”ì 
"""

import pandas as pd
import requests
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Optional, Tuple
import logging
from deltalake import DeltaTable, write_deltalake
import pyarrow as pa
from google.cloud import storage
from dotenv import load_dotenv
import re
import time
import os

load_dotenv()
logger = logging.getLogger(__name__)

class SP500MembershipTracker:
    """S&P 500 í¸ì…/í‡´ì¶œ ì´ë ¥ ì¶”ì  í´ë˜ìŠ¤"""
    
    def __init__(self, gcs_bucket: str, gcs_path: str = "stock_dashboard/membership"):
        """
        ì´ˆê¸°í™”
        
        Args:
            gcs_bucket: GCS ë²„í‚· ì´ë¦„
            gcs_path: GCS ê²½ë¡œ
        """
        self.gcs_bucket = gcs_bucket
        self.gcs_path = gcs_path
        self.client = storage.Client()
        
        # Delta Table ê²½ë¡œ
        self.membership_changes_path = f"gs://{gcs_bucket}/{gcs_path}/sp500_membership_changes"
        self.membership_daily_path = f"gs://{gcs_bucket}/{gcs_path}/sp500_membership_daily"
    
    def get_sp500_for_year(self, year: int) -> pd.DataFrame:
        """íŠ¹ì • ì—°ë„ì˜ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ (Wikipediaì—ì„œ ì‹¤ì œ ë°ì´í„°)"""
        logger.info(f"ğŸ“‹ {year}ë…„ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì¤‘...")
        
        # ëª¨ë“  ì—°ë„ì— ëŒ€í•´ Wikipediaì—ì„œ ì‹¤ì œ S&P 500 ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        try:
            logger.info("ğŸŒ Wikipediaì—ì„œ í˜„ì¬ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì¤‘...")
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            
            # User-Agent í—¤ë” ì¶”ê°€ (403 Forbidden ë°©ì§€)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            tables = pd.read_html(url, header=0)
            
            # ì²« ë²ˆì§¸ í…Œì´ë¸”ì´ S&P 500 êµ¬ì„± ì¢…ëª© í…Œì´ë¸”
            sp500_df = tables[0]
            sp500_df.columns = sp500_df.columns.str.strip()
            
            # Symbol ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if 'Symbol' in sp500_df.columns:
                logger.info(f"âœ… Wikipediaì—ì„œ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ: {len(sp500_df)}ê°œ")
                return sp500_df
            else:
                logger.warning("âš ï¸ Wikipedia í…Œì´ë¸”ì—ì„œ Symbol ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                logger.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(sp500_df.columns)}")
                
        except Exception as e:
            logger.error(f"âŒ Wikipedia ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ëŒ€ì•ˆ ë°©ë²•ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # ëŒ€ì•ˆ: requests + BeautifulSoup ì‚¬ìš©
            try:
                import requests
                from bs4 import BeautifulSoup
                
                logger.info("ğŸŒ requests + BeautifulSoupìœ¼ë¡œ ì¬ì‹œë„...")
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                table = soup.find('table', {'class': 'wikitable'})
                
                if table:
                    # í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ë³€í™˜
                    sp500_df = pd.read_html(str(table), header=0)[0]
                    sp500_df.columns = sp500_df.columns.str.strip()
                    
                    if 'Symbol' in sp500_df.columns:
                        logger.info(f"âœ… ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ: {len(sp500_df)}ê°œ")
                        return sp500_df
                
            except Exception as e2:
                logger.error(f"âŒ ëŒ€ì•ˆ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
        
        # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì—ëŸ¬ ë°œìƒ
        raise Exception("S&P 500 êµ¬ì„± ì¢…ëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    
    def get_current_sp500_from_wikipedia(self) -> pd.DataFrame:
        """í˜„ì¬ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ (2024ë…„ ê¸°ì¤€)"""
        return self.get_sp500_for_year(2024)
    
    def scrape_wikipedia_changes(self, start_year: int = 2000) -> pd.DataFrame:
        """
        Wikipediaì˜ S&P 500 ë³€ê²½ ì´ë ¥ í˜ì´ì§€ë“¤ì„ ìŠ¤í¬ë˜í•‘ - ê°œì„ ëœ ë²„ì „
        
        Args:
            start_year: ìˆ˜ì§‘ ì‹œì‘ ì—°ë„
            
        Returns:
            pd.DataFrame: í¸ì…/í‡´ì¶œ ì´ë ¥ ë°ì´í„°
        """
        logger.info(f"ğŸ“Š Wikipedia S&P 500 ë³€ê²½ ì´ë ¥ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ì—°ë„: {start_year}~)")
        
        changes_list = []
        
        # ì—¬ëŸ¬ Wikipedia í˜ì´ì§€ ì‹œë„
        urls_to_try = [
            "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies#Changes_to_the_S%26P_500",
            "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies",
            "https://en.wikipedia.org/wiki/S%26P_500_Index#Changes_to_the_S%26P_500"
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        for url_idx, url in enumerate(urls_to_try):
            try:
                logger.info(f"ğŸŒ Wikipedia í˜ì´ì§€ ì ‘ê·¼ ì‹œë„ {url_idx + 1}: {url}")
                
                # requestsë¡œ ì§ì ‘ ì ‘ê·¼
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # "Changes to the S&P 500" ì„¹ì…˜ ì°¾ê¸°
                changes_section = soup.find('span', {'id': 'Changes_to_the_S.26P_500'})
                if not changes_section:
                    changes_section = soup.find('span', {'id': 'Changes_to_the_S&P_500'})
                if not changes_section:
                    changes_section = soup.find('h2', string=lambda text: text and 'Changes' in text)
                
                if changes_section:
                    logger.info("âœ… 'Changes to the S&P 500' ì„¹ì…˜ ë°œê²¬")
                    
                    # ì„¹ì…˜ ë‹¤ìŒì˜ í…Œì´ë¸”ë“¤ ì°¾ê¸°
                    tables = changes_section.find_all_next('table')
                    
                    for table_idx, table in enumerate(tables[:3]):  # ìµœëŒ€ 3ê°œ í…Œì´ë¸”ë§Œ í™•ì¸
                        try:
                            # í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ë³€í™˜
                            table_df = pd.read_html(str(table), header=0)[0]
                            logger.info(f"ğŸ“Š í…Œì´ë¸” {table_idx + 1} ì»¬ëŸ¼: {list(table_df.columns)}")
                            
                            # ë³€ê²½ ì´ë ¥ í…Œì´ë¸”ì¸ì§€ í™•ì¸
                            if self._is_changes_table(table_df):
                                logger.info(f"âœ… ë³€ê²½ ì´ë ¥ í…Œì´ë¸” ë°œê²¬: í…Œì´ë¸” {table_idx + 1}")
                                changes_list.extend(self._parse_changes_table(table_df, start_year))
                                break
                                
                        except Exception as e:
                            logger.debug(f"í…Œì´ë¸” {table_idx + 1} íŒŒì‹± ì‹¤íŒ¨: {e}")
                            continue
                    
                    if changes_list:
                        break  # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±í–ˆìœ¼ë©´ ë‹¤ë¥¸ URL ì‹œë„í•˜ì§€ ì•ŠìŒ
                else:
                    logger.warning(f"âš ï¸ 'Changes to the S&P 500' ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                
                time.sleep(2)  # API ì œí•œ ë°©ì§€
                
            except Exception as e:
                logger.error(f"âŒ URL {url_idx + 1} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                continue
        
        if changes_list:
            changes_df = pd.DataFrame(changes_list)
            # ì¤‘ë³µ ì œê±°
            changes_df = changes_df.drop_duplicates(subset=['effective_date', 'action', 'ticker'])
            changes_df = changes_df.sort_values('effective_date')
            logger.info(f"âœ… Wikipedia ë³€ê²½ ì´ë ¥ ìˆ˜ì§‘ ì™„ë£Œ: {len(changes_df)}ê°œ ë³€ê²½ì‚¬í•­")
            return changes_df
        else:
            logger.warning("âš ï¸ Wikipediaì—ì„œ ë³€ê²½ ì´ë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame(columns=['effective_date', 'action', 'ticker', 'description', 'year'])
    
    def _is_changes_table(self, df: pd.DataFrame) -> bool:
        """í…Œì´ë¸”ì´ ë³€ê²½ ì´ë ¥ í…Œì´ë¸”ì¸ì§€ í™•ì¸"""
        if df.empty or len(df.columns) < 2:
            return False
        
        # ì»¬ëŸ¼ëª…ì—ì„œ ë‚ ì§œ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        date_indicators = ['date', 'Date', 'DATE', 'effective', 'Effective', 'change', 'Change']
        action_indicators = ['added', 'removed', 'replaced', 'change', 'Change', 'company', 'Company']
        
        columns_str = ' '.join(str(col).lower() for col in df.columns)
        
        has_date = any(indicator in columns_str for indicator in date_indicators)
        has_action = any(indicator in columns_str for indicator in action_indicators)
        
        return has_date or has_action
    
    def _parse_changes_table(self, df: pd.DataFrame, start_year: int) -> List[Dict]:
        """ë³€ê²½ ì´ë ¥ í…Œì´ë¸” íŒŒì‹±"""
        changes_list = []
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
        date_col = None
        for col in df.columns:
            if any(indicator in str(col).lower() for indicator in ['date', 'effective', 'change']):
                date_col = col
                break
        
        if not date_col:
            logger.warning("âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return changes_list
        
        logger.info(f"ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ ë°œê²¬: {date_col}")
        
        # ë‚ ì§œ íŒŒì‹±
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col])
        
        # í¸ì…/í‡´ì¶œ ì •ë³´ ì¶”ì¶œ
        for _, row in df.iterrows():
            date_val = row[date_col]
            if pd.notna(date_val) and date_val.year >= start_year:
                # ê° ì»¬ëŸ¼ì—ì„œ í¸ì…/í‡´ì¶œ ì •ë³´ íŒŒì‹±
                for col in df.columns:
                    if col != date_col:
                        cell_value = str(row[col])
                        
                        # í¸ì… ì •ë³´ ì¶”ì¶œ
                        if any(keyword in cell_value.lower() for keyword in ['added', 'add', 'replaced', 'replace']):
                            tickers = self._extract_tickers_from_text(cell_value)
                            for ticker in tickers:
                                changes_list.append({
                                    'effective_date': date_val.date(),
                                    'action': 'add',
                                    'ticker': ticker,
                                    'description': cell_value,
                                    'year': date_val.year
                                })
                        
                        # í‡´ì¶œ ì •ë³´ ì¶”ì¶œ
                        elif any(keyword in cell_value.lower() for keyword in ['removed', 'remove', 'replaced', 'replace']):
                            tickers = self._extract_tickers_from_text(cell_value)
                            for ticker in tickers:
                                changes_list.append({
                                    'effective_date': date_val.date(),
                                    'action': 'remove',
                                    'ticker': ticker,
                                    'description': cell_value,
                                    'year': date_val.year
                                })
        
        return changes_list
    
    def _extract_tickers_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ ì¶”ì¶œ - ê°œì„ ëœ ë¡œì§"""
        tickers = []
        
        # ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ ì œì™¸
        exclude_words = {
            'THE', 'AND', 'FOR', 'INC', 'CORP', 'LLC', 'LTD', 'CO', 'COMPANY', 
            'GROUP', 'HOLDINGS', 'INTERNATIONAL', 'SYSTEMS', 'SOLUTIONS',
            'TECHNOLOGIES', 'COMMUNICATIONS', 'FINANCIAL', 'SERVICES',
            'HEALTHCARE', 'PHARMACEUTICAL', 'ENERGY', 'UTILITIES',
            'REAL', 'ESTATE', 'INVESTMENT', 'MANAGEMENT', 'PARTNERS',
            'ADDED', 'REMOVED', 'REPLACED', 'CHANGE', 'CHANGES'
        }
        
        # í‹°ì»¤ íŒ¨í„´ë“¤
        patterns = [
            r'\b[A-Z]{1,5}\b',  # ê¸°ë³¸ í‹°ì»¤ íŒ¨í„´
            r'\b[A-Z]{2,4}\b',  # 2-4ìë¦¬ í‹°ì»¤
            r'\b[A-Z]{1,3}[0-9]{1,2}\b',  # ë¬¸ì+ìˆ«ì ì¡°í•©
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.upper())
            for match in matches:
                # ì œì™¸ ë‹¨ì–´ í•„í„°ë§
                if (match not in exclude_words and 
                    len(match) >= 2 and 
                    len(match) <= 5 and
                    not match.isdigit()):
                    tickers.append(match)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return sorted(list(set(tickers)))
    
    def create_manual_membership_changes(self) -> pd.DataFrame:
        """
        ìˆ˜ë™ìœ¼ë¡œ ì£¼ìš” í¸ì…/í‡´ì¶œ ì´ë ¥ ìƒì„± - ê°œì„ ëœ ë²„ì „
        Wikipedia ìŠ¤í¬ë˜í•‘ì´ ì–´ë ¤ìš´ ê²½ìš° ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš©
        """
        logger.info("ğŸ“ ì£¼ìš” S&P 500 í¸ì…/í‡´ì¶œ ì´ë ¥ ìˆ˜ë™ ìƒì„± ì¤‘...")
        
        # ì£¼ìš” í¸ì…/í‡´ì¶œ ì´ë ¥ (ì‹¤ì œ í¸ì…ì¼ ê¸°ì¤€) - ìˆ˜ì •ëœ ë¶€ë¶„
        changes_data = [
            # 1990ë…„ëŒ€ ì£¼ìš” í¸ì…
            {'effective_date': '1997-05-15', 'action': 'add', 'ticker': 'AMZN', 'description': 'Amazon.com Inc. added'},
            {'effective_date': '1999-01-22', 'action': 'add', 'ticker': 'NVDA', 'description': 'NVIDIA Corporation added'},
            
            # 2000ë…„ëŒ€ ì£¼ìš” í¸ì…
            {'effective_date': '2000-01-01', 'action': 'add', 'ticker': 'AAPL', 'description': 'Apple Inc. added'},
            {'effective_date': '2004-08-19', 'action': 'add', 'ticker': 'GOOGL', 'description': 'Google Inc. added'},
            {'effective_date': '2004-08-19', 'action': 'add', 'ticker': 'GOOG', 'description': 'Google Inc. Class A added'},
            
            # 2010ë…„ëŒ€ ì£¼ìš” í¸ì…
            {'effective_date': '2012-05-18', 'action': 'add', 'ticker': 'META', 'description': 'Facebook Inc. added'},
            {'effective_date': '2013-05-31', 'action': 'add', 'ticker': 'TSLA', 'description': 'Tesla Inc. added'},
            {'effective_date': '2014-03-20', 'action': 'add', 'ticker': 'FB', 'description': 'Facebook Inc. (old ticker) added'},
            
            # 2020ë…„ëŒ€ ì£¼ìš” í¸ì…
            {'effective_date': '2020-12-21', 'action': 'add', 'ticker': 'TSM', 'description': 'Taiwan Semiconductor added'},
            {'effective_date': '2021-03-22', 'action': 'add', 'ticker': 'PLTR', 'description': 'Palantir Technologies added'},
            
            # ì£¼ìš” í‡´ì¶œ ì´ë ¥
            {'effective_date': '2008-09-15', 'action': 'remove', 'ticker': 'LEH', 'description': 'Lehman Brothers removed'},
            {'effective_date': '2009-06-01', 'action': 'remove', 'ticker': 'GM', 'description': 'General Motors removed'},
            {'effective_date': '2018-06-26', 'action': 'remove', 'ticker': 'GE', 'description': 'General Electric removed'},
            {'effective_date': '2020-08-31', 'action': 'remove', 'ticker': 'ETFC', 'description': 'E*TRADE removed'},
            
            # ìµœê·¼ ì£¼ìš” ë³€ê²½ì‚¬í•­ (2022-2024)
            {'effective_date': '2022-03-18', 'action': 'add', 'ticker': 'CEG', 'description': 'Constellation Energy added'},
            {'effective_date': '2022-06-06', 'action': 'add', 'ticker': 'ENPH', 'description': 'Enphase Energy added'},
            {'effective_date': '2023-03-20', 'action': 'add', 'ticker': 'SEDG', 'description': 'SolarEdge Technologies added'},
            {'effective_date': '2023-06-20', 'action': 'add', 'ticker': 'GEHC', 'description': 'GE HealthCare added'},
            {'effective_date': '2024-01-22', 'action': 'add', 'ticker': 'SMCI', 'description': 'Super Micro Computer added'},
        ]
        
        changes_df = pd.DataFrame(changes_data)
        changes_df['effective_date'] = pd.to_datetime(changes_df['effective_date']).dt.date
        changes_df['year'] = pd.to_datetime(changes_df['effective_date']).dt.year
        
        logger.info(f"âœ… ìˆ˜ë™ í¸ì…/í‡´ì¶œ ì´ë ¥ ìƒì„± ì™„ë£Œ: {len(changes_df)}ê°œ")
        return changes_df
    
    def save_membership_changes(self, changes_df: pd.DataFrame):
        """í¸ì…/í‡´ì¶œ ì´ë ¥ì„ Delta Tableì— ì €ì¥"""
        logger.info("ğŸ’¾ í¸ì…/í‡´ì¶œ ì´ë ¥ì„ Delta Tableì— ì €ì¥ ì¤‘...")
        
        if changes_df.empty:
            logger.warning("ì €ì¥í•  ë³€ê²½ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            try:
                existing_delta = DeltaTable(self.membership_changes_path)
                existing_df = existing_delta.to_pandas()
                
                if not existing_df.empty:
                    # ì¤‘ë³µ ì œê±° (ê°™ì€ ë‚ ì§œ, ì•¡ì…˜, í‹°ì»¤)
                    changes_df = changes_df[~changes_df.set_index(['effective_date', 'action', 'ticker']).index.isin(
                        existing_df.set_index(['effective_date', 'action', 'ticker']).index
                    )]
                    
                    if changes_df.empty:
                        logger.info("ìƒˆë¡œìš´ ë³€ê²½ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                        return
                    
                    # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ê²°í•©
                    changes_df = pd.concat([existing_df, changes_df], ignore_index=True)
                    mode = "overwrite"
                else:
                    mode = "overwrite"
                    
            except Exception:
                mode = "overwrite"
            
            # Delta Tableì— ì €ì¥
            arrow_table = pa.Table.from_pandas(changes_df)
            
            write_deltalake(
                self.membership_changes_path,
                arrow_table,
                mode=mode,
                partition_by=["year"],  # ì—°ë„ë³„ íŒŒí‹°ì…”ë‹
                configuration={
                    "delta.autoOptimize.optimizeWrite": "true",
                    "delta.autoOptimize.autoCompact": "true"
                }
            )
            
            logger.info(f"âœ… í¸ì…/í‡´ì¶œ ì´ë ¥ ì €ì¥ ì™„ë£Œ: {len(changes_df)}ê°œ")
            logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.membership_changes_path}")
            
        except Exception as e:
            logger.error(f"âŒ í¸ì…/í‡´ì¶œ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def generate_daily_membership(self, start_date: date, end_date: date) -> pd.DataFrame:
        """
        ì¼ìë³„ ë©¤ë²„ì‹­ ìŠ¤ëƒ…ìƒ· ìƒì„±
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            pd.DataFrame: ì¼ìë³„ ë©¤ë²„ì‹­ ë°ì´í„°
        """
        logger.info(f"ğŸ“… ì¼ìë³„ ë©¤ë²„ì‹­ ìŠ¤ëƒ…ìƒ· ìƒì„± ì¤‘... ({start_date} ~ {end_date})")
        
        try:
            # í¸ì…/í‡´ì¶œ ì´ë ¥ ë¡œë“œ
            changes_delta = DeltaTable(self.membership_changes_path)
            changes_df = changes_delta.to_pandas()
            
            if changes_df.empty:
                logger.warning("í¸ì…/í‡´ì¶œ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            changes_df['effective_date'] = pd.to_datetime(changes_df['effective_date']).dt.date
            
            # ê¸°ì¤€ ì—°ë„ì˜ êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ (í•œ ë²ˆë§Œ í˜¸ì¶œ) - ìˆ˜ì •ëœ ë¶€ë¶„
            base_year = start_date.year
            logger.info(f"ğŸ“‹ {base_year}ë…„ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì¤‘... (ë…„ë„ë³„ ì‹¤ì œ ì¡´ì¬ ì¢…ëª©)")
            current_sp500 = self.get_sp500_for_year(base_year)
            base_tickers = set(current_sp500['Symbol'].tolist())
            logger.info(f"âœ… {base_year}ë…„ S&P 500 êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ: {len(base_tickers)}ê°œ")
            
            # ì¼ìë³„ ë©¤ë²„ì‹­ ìƒì„±
            date_list = []
            current_date = start_date
            while current_date <= end_date:
                if current_date.weekday() < 5:  # í‰ì¼ë§Œ
                    date_list.append(current_date)
                current_date += timedelta(days=1)
            
            daily_membership_list = []
            
            for target_date in date_list:
                # í•´ë‹¹ ë‚ ì§œê¹Œì§€ì˜ í¸ì…/í‡´ì¶œ ì´ë ¥ ì ìš©
                relevant_changes = changes_df[changes_df['effective_date'] <= target_date]
                
                # í¸ì…ëœ ì¢…ëª©ë“¤
                added_tickers = set(relevant_changes[relevant_changes['action'] == 'add']['ticker'].tolist())
                
                # í‡´ì¶œëœ ì¢…ëª©ë“¤
                removed_tickers = set(relevant_changes[relevant_changes['action'] == 'remove']['ticker'].tolist())
                
                # Point-in-Time: í•´ë‹¹ ë‚ ì§œê¹Œì§€ í¸ì…ëœ ì¢…ëª©ë“¤ë§Œ (í‡´ì¶œëœ ì¢…ëª© ì œì™¸)
                # 2000ë…„ 1ì›” 1ì¼ì´ë©´ 2000ë…„ 1ì›” 1ì¼ê¹Œì§€ í¸ì…ëœ ì¢…ëª©ë“¤ë§Œ
                # í¸ì… ì´ë ¥ì´ ì—†ëŠ” ì¢…ëª©ë“¤ì€ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ (2000ë…„ ê¸°ì¤€ ì¢…ëª©ë“¤)
                # base_tickersëŠ” ì´ë¯¸ ìœ„ì—ì„œ í•œ ë²ˆë§Œ ê³„ì‚°ë¨ - ìˆ˜ì •ëœ ë¶€ë¶„
                
                # í•´ë‹¹ ë‚ ì§œê¹Œì§€ í¸ì…ëœ ì¢…ëª©ë“¤ë§Œ í•„í„°ë§
                valid_added_tickers = set()
                for ticker in added_tickers:
                    ticker_changes = relevant_changes[relevant_changes['ticker'] == ticker]
                    add_changes = ticker_changes[ticker_changes['action'] == 'add']
                    if not add_changes.empty:
                        # í•´ë‹¹ ë‚ ì§œ ì´ì „ì— í¸ì…ëœ ì¢…ëª©ë§Œ í¬í•¨
                        earliest_add = add_changes['effective_date'].min()
                        if earliest_add <= target_date:
                            valid_added_tickers.add(ticker)
                
                daily_tickers = (base_tickers | valid_added_tickers) - removed_tickers
                
                # ê° ì¢…ëª©ë³„ë¡œ ë©¤ë²„ì‹­ ì •ë³´ ìƒì„±
                for ticker in daily_tickers:
                    # í¸ì…ì¼/í‡´ì¶œì¼ ê³„ì‚°
                    ticker_adds = relevant_changes[
                        (relevant_changes['ticker'] == ticker) & 
                        (relevant_changes['action'] == 'add')
                    ]
                    ticker_removes = relevant_changes[
                        (relevant_changes['ticker'] == ticker) & 
                        (relevant_changes['action'] == 'remove')
                    ]
                    
                    in_dt = ticker_adds['effective_date'].min() if not ticker_adds.empty else None
                    out_dt = ticker_removes['effective_date'].min() if not ticker_removes.empty else None
                    
                    daily_membership_list.append({
                        'date': target_date,
                        'ticker': ticker,
                        'in_dt': in_dt if in_dt is not None else target_date,  # None ëŒ€ì‹  target_date ì‚¬ìš©
                        'out_dt': out_dt if out_dt is not None else pd.NaT,  # None ëŒ€ì‹  NaT ì‚¬ìš©
                        'is_member': True
                    })
            
            daily_membership_df = pd.DataFrame(daily_membership_list)
            
            logger.info(f"âœ… ì¼ìë³„ ë©¤ë²„ì‹­ ìŠ¤ëƒ…ìƒ· ìƒì„± ì™„ë£Œ: {len(daily_membership_df)}ê°œ ë ˆì½”ë“œ")
            return daily_membership_df
            
        except Exception as e:
            logger.error(f"âŒ ì¼ìë³„ ë©¤ë²„ì‹­ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def save_daily_membership(self, daily_membership_df: pd.DataFrame):
        """ì¼ìë³„ ë©¤ë²„ì‹­ì„ Delta Tableì— ì €ì¥"""
        logger.info("ğŸ’¾ ì¼ìë³„ ë©¤ë²„ì‹­ì„ Delta Tableì— ì €ì¥ ì¤‘...")
        
        if daily_membership_df.empty:
            logger.warning("ì €ì¥í•  ì¼ìë³„ ë©¤ë²„ì‹­ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë° ë®ì–´ì“°ê¸°
            arrow_table = pa.Table.from_pandas(daily_membership_df)
            
            write_deltalake(
                self.membership_daily_path,
                arrow_table,
                mode="overwrite",  # ì „ì²´ ë®ì–´ì“°ê¸°
                partition_by=["date"],  # ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹
                configuration={
                    "delta.autoOptimize.optimizeWrite": "true",
                    "delta.autoOptimize.autoCompact": "true"
                }
            )
            
            logger.info(f"âœ… ì¼ìë³„ ë©¤ë²„ì‹­ ì €ì¥ ì™„ë£Œ: {len(daily_membership_df)}ê°œ")
            logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.membership_daily_path}")
            
        except Exception as e:
            logger.error(f"âŒ ì¼ìë³„ ë©¤ë²„ì‹­ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def get_daily_membership(self, target_date: date) -> pd.DataFrame:
        """íŠ¹ì • ë‚ ì§œì˜ ë©¤ë²„ì‹­ ì¡°íšŒ"""
        try:
            daily_delta = DeltaTable(self.membership_daily_path)
            daily_df = daily_delta.to_pandas()
            
            if not daily_df.empty:
                daily_df['date'] = pd.to_datetime(daily_df['date']).dt.date
                target_membership = daily_df[daily_df['date'] == target_date]
                return target_membership
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"âŒ ì¼ìë³„ ë©¤ë²„ì‹­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def get_membership_for_date_range(self, start_date: date, end_date: date) -> pd.DataFrame:
        """
        íŠ¹ì • ë‚ ì§œ ë²”ìœ„ì˜ ë©¤ë²„ì‹­ ì •ë³´ ì¡°íšŒ
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            pd.DataFrame: í•´ë‹¹ ê¸°ê°„ì˜ ë©¤ë²„ì‹­ ì •ë³´
        """
        try:
            daily_delta = DeltaTable(self.membership_daily_path)
            daily_df = daily_delta.to_pandas()
            
            if not daily_df.empty:
                daily_df['date'] = pd.to_datetime(daily_df['date']).dt.date
                # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
                membership_df = daily_df[
                    (daily_df['date'] >= start_date) & 
                    (daily_df['date'] <= end_date)
                ]
                return membership_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"âŒ ë©¤ë²„ì‹­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def get_tickers_for_backfill(self, start_date: date, end_date: date) -> Dict[str, Dict[str, date]]:
        """
        ë°±í•„ì„ ìœ„í•œ ì¢…ëª©ë³„ í¸ì…ì¼ ì •ë³´ ì¡°íšŒ
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            Dict[str, Dict[str, date]]: {ticker: {'in_date': date, 'out_date': date}}
        """
        logger.info(f"ğŸ“‹ ë°±í•„ìš© ì¢…ëª©ë³„ í¸ì…ì¼ ì •ë³´ ì¡°íšŒ ì¤‘... ({start_date} ~ {end_date})")
        
        try:
            # í•´ë‹¹ ê¸°ê°„ì˜ ë©¤ë²„ì‹­ ì •ë³´ ì¡°íšŒ
            membership_df = self.get_membership_for_date_range(start_date, end_date)
            
            if membership_df.empty:
                logger.warning("ë©¤ë²„ì‹­ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            # ì¢…ëª©ë³„ í¸ì…/í‡´ì¶œì¼ ì •ë¦¬
            ticker_info = {}
            
            for ticker in membership_df['ticker'].unique():
                ticker_data = membership_df[membership_df['ticker'] == ticker]
                
                # í¸ì…ì¼ (í•´ë‹¹ ê¸°ê°„ ë‚´ ìµœì´ˆ ë“±ì¥ì¼)
                in_date = ticker_data['date'].min()
                
                # í‡´ì¶œì¼ (í•´ë‹¹ ê¸°ê°„ ë‚´ ë§ˆì§€ë§‰ ë“±ì¥ì¼ ì´í›„)
                out_date = ticker_data['date'].max()
                
                # í‡´ì¶œ ì—¬ë¶€ í™•ì¸ (ë§ˆì§€ë§‰ ë‚ ì§œê°€ end_dateë³´ë‹¤ ì´ì „ì´ë©´ í‡´ì¶œ)
                if out_date < end_date:
                    # ì‹¤ì œ í‡´ì¶œì¼ í™•ì¸
                    out_dt = ticker_data['out_dt'].iloc[0]
                    if pd.notna(out_dt):
                        out_date = out_dt
                
                ticker_info[ticker] = {
                    'in_date': in_date,
                    'out_date': out_date if out_date < end_date else None
                }
            
            logger.info(f"âœ… ë°±í•„ìš© ì¢…ëª© ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {len(ticker_info)}ê°œ ì¢…ëª©")
            return ticker_info
            
        except Exception as e:
            logger.error(f"âŒ ë°±í•„ìš© ì¢…ëª© ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def run_membership_setup(self, start_date: date, end_date: date, use_manual: bool = True):
        """
        ë©¤ë²„ì‹­ ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸° ì„¤ì •
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            use_manual: ìˆ˜ë™ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€
        """
        logger.info("=" * 80)
        logger.info("ğŸ“‹ S&P 500 ë©¤ë²„ì‹­ ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸° ì„¤ì •")
        logger.info("=" * 80)
        logger.info(f" ì„¤ì • ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info(f" ë°ì´í„° ì†ŒìŠ¤: {'ìˆ˜ë™' if use_manual else 'Wikipedia ìŠ¤í¬ë˜í•‘'}")
        
        try:
            # 1. í¸ì…/í‡´ì¶œ ì´ë ¥ ìƒì„±
            if use_manual:
                changes_df = self.create_manual_membership_changes()
            else:
                changes_df = self.scrape_wikipedia_changes()
            
            # 2. í¸ì…/í‡´ì¶œ ì´ë ¥ ì €ì¥
            if not changes_df.empty:
                self.save_membership_changes(changes_df)
            
            # 3. ì¼ìë³„ ë©¤ë²„ì‹­ ìƒì„±
            daily_membership_df = self.generate_daily_membership(start_date, end_date)
            
            # 4. ì¼ìë³„ ë©¤ë²„ì‹­ ì €ì¥
            if not daily_membership_df.empty:
                self.save_daily_membership(daily_membership_df)
            
            # 5. ìš”ì•½ ì •ë³´ ì¶œë ¥
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“ˆ ë©¤ë²„ì‹­ ì¶”ì  ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            logger.info("=" * 80)
            logger.info(f" í¸ì…/í‡´ì¶œ ì´ë ¥: {len(changes_df)}ê°œ")
            logger.info(f" ì¼ìë³„ ë©¤ë²„ì‹­: {len(daily_membership_df)}ê°œ ë ˆì½”ë“œ")
            logger.info(f" ê¸°ê°„: {start_date} ~ {end_date}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"âŒ ë©¤ë²„ì‹­ ì‹œìŠ¤í…œ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="S&P 500 ë©¤ë²„ì‹­ ì¶”ì  ì‹œìŠ¤í…œ")
    parser.add_argument("--start-date", type=str, required=True, help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, required=True, help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--use-manual", action="store_true", help="ìˆ˜ë™ ë°ì´í„° ì‚¬ìš©")
    
    args = parser.parse_args()
    
    # GCS ì„¤ì •
    gcs_bucket = os.getenv("GCS_BUCKET", "your-stock-dashboard-bucket")
    tracker = SP500MembershipTracker(gcs_bucket)
    
    # ë‚ ì§œ íŒŒì‹±
    start_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
    end_date = datetime.strptime(args.end_date, "%Y-%m-%d").date()
    
    try:
        tracker.run_membership_setup(start_date, end_date, args.use_manual)
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()
