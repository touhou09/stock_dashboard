"""
Bronze Layer ë°ì´í„° ì €ì¥ ëª¨ë“ˆ
- Delta Lake í…Œì´ë¸” ì €ì¥
- GCS ì—°ë™
"""

import pandas as pd
from datetime import datetime, date
from typing import List
import logging
from deltalake import DeltaTable, write_deltalake, WriterProperties
from google.cloud import storage
import pyarrow as pa
import pyarrow.parquet as pq

logger = logging.getLogger(__name__)

class DeltaStorageManager:
    """Delta Lake ì €ì¥ ê´€ë¦¬ì"""
    
    def __init__(self, gcs_bucket: str, gcs_path: str = "stock_dashboard/bronze"):
        """
        ì €ì¥ ê´€ë¦¬ì ì´ˆê¸°í™”
        
        Args:
            gcs_bucket: GCS ë²„í‚· ì´ë¦„
            gcs_path: GCS ë‚´ ê²½ë¡œ
        """
        self.gcs_bucket = gcs_bucket
        self.gcs_path = gcs_path
        self.storage_client = storage.Client()
        self.bucket = self.storage_client.bucket(gcs_bucket)
        
        # Delta Table ê²½ë¡œ ì„¤ì •
        self.price_table_path = f"gs://{gcs_bucket}/{gcs_path}/bronze_price_daily"
        self.dividend_events_table_path = f"gs://{gcs_bucket}/{gcs_path}/bronze_dividend_events"
    
    def check_existing_data(self, table_path: str, target_date: datetime.date) -> bool:
        """íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            delta_table = DeltaTable(table_path)
            existing_df = delta_table.to_pandas()
            
            # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if 'date' in existing_df.columns:
                # ë‚ ì§œ í˜•ì‹ í†µì¼
                existing_df['date'] = pd.to_datetime(existing_df['date']).dt.date
                target_date_str = target_date.strftime('%Y-%m-%d')
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                has_data = (existing_df['date'] == target_date).any()
                
                if has_data:
                    count = (existing_df['date'] == target_date).sum()
                    logger.info(f"ğŸ“… {target_date_str} ë‚ ì§œì˜ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {count}í–‰")
                    return True
                else:
                    logger.info(f"ğŸ“… {target_date_str} ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return False
            else:
                logger.info("ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.info(f"ğŸ“… ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨ (í…Œì´ë¸”ì´ ì—†ì„ ìˆ˜ ìˆìŒ): {e}")
            return False
    
    def save_price_data_to_delta(self, all_daily_data: List[pd.DataFrame], target_date: datetime.date):
        """ê°€ê²© ë°ì´í„°ë¥¼ Delta Tableì— ì €ì¥ (Bronze ìŠ¤í‚¤ë§ˆ)"""
        logger.info(f"\nğŸ’¾ ê°€ê²© ë°ì´í„°ë¥¼ Bronze Delta Tableì— ì €ì¥ ì¤‘...")
        
        if not all_daily_data:
            logger.warning("ì €ì¥í•  ê°€ê²© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë‚ ì§œ ì¤‘ë³µ í™•ì¸
        if self.check_existing_data(self.price_table_path, target_date):
            logger.warning(f"âš ï¸ {target_date} ë‚ ì§œì˜ ê°€ê²© ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # pandas DataFrame ê²°í•©
        combined_df = pd.concat(all_daily_data, ignore_index=True)
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (Bronze ìŠ¤í‚¤ë§ˆ)
        bronze_columns = ['date', 'ticker', 'open', 'high', 'low', 'close', 'volume', 'adj_close', 'ingest_at']
        combined_df = combined_df[bronze_columns]
        
        try:
            # Delta Tableì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            delta_table = DeltaTable(self.price_table_path)
            mode = "append"
            logger.info("âœ… ê¸°ì¡´ Bronze ê°€ê²© í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€")
        except Exception:
            mode = "overwrite"
            logger.info("ğŸ†• ìƒˆë¡œìš´ Bronze ê°€ê²© í…Œì´ë¸” ìƒì„±")
        
        # [ìˆ˜ì •] deltalake 1.0+ WriterPropertiesë¡œ zstd ì••ì¶• ì„¤ì •
        arrow_table = pa.Table.from_pandas(combined_df)
        
        # zstd ì••ì¶• ì„¤ì •
        writer_props = WriterProperties(
            compression='ZSTD',
            compression_level=5
        )
        
        # Delta Tableì— ì €ì¥
        write_deltalake(
            self.price_table_path,
            arrow_table,
            mode=mode,
            partition_by=["date"],  # ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹
            writer_properties=writer_props,  # [ìˆ˜ì •] zstd ì••ì¶• ì ìš©
            configuration={
                "delta.dataSkippingStatsColumns": "ticker,close",  # í†µê³„ ìµœì í™”
                "delta.autoOptimize.optimizeWrite": "true",        # ìë™ ìµœì í™”
                "delta.autoOptimize.autoCompact": "true"           # ìë™ ì••ì¶•
            }
        )
        
        logger.info(f"âœ… Bronze ê°€ê²© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(combined_df)}í–‰")
        logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.price_table_path}")
    
    def save_dividend_events_to_delta(self, dividend_events_df: pd.DataFrame):
        """ë°°ë‹¹ ì´ë²¤íŠ¸ë¥¼ Delta Tableì— ì €ì¥ (Bronze ìŠ¤í‚¤ë§ˆ)"""
        logger.info(f"\nğŸ’¾ ë°°ë‹¹ ì´ë²¤íŠ¸ë¥¼ Bronze Delta Tableì— ì €ì¥ ì¤‘...")
        
        if dividend_events_df.empty:
            logger.warning("ì €ì¥í•  ë°°ë‹¹ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        
        try:
            # Delta Tableì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            delta_table = DeltaTable(self.dividend_events_table_path)
            mode = "append"
            logger.info("âœ… ê¸°ì¡´ Bronze ë°°ë‹¹ ì´ë²¤íŠ¸ í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€")
        except Exception:
            mode = "overwrite"
            logger.info("ğŸ†• ìƒˆë¡œìš´ Bronze ë°°ë‹¹ ì´ë²¤íŠ¸ í…Œì´ë¸” ìƒì„±")
        
        # [ìˆ˜ì •] deltalake 1.0+ WriterPropertiesë¡œ zstd ì••ì¶• ì„¤ì •
        arrow_table = pa.Table.from_pandas(dividend_events_df)
        
        # zstd ì••ì¶• ì„¤ì •
        writer_props = WriterProperties(
            compression='ZSTD',
            compression_level=5
        )
        
        # Delta Tableì— ì €ì¥
        write_deltalake(
            self.dividend_events_table_path,
            arrow_table,
            mode=mode,
            partition_by=["date"],  # ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹
            writer_properties=writer_props,  # [ìˆ˜ì •] zstd ì••ì¶• ì ìš©
            configuration={
                "delta.dataSkippingStatsColumns": "ticker,amount",  # í†µê³„ ìµœì í™”
                "delta.autoOptimize.optimizeWrite": "true",         # ìë™ ìµœì í™”
                "delta.autoOptimize.autoCompact": "true"            # ìë™ ì••ì¶•
            }
        )
        
        logger.info(f"âœ… Bronze ë°°ë‹¹ ì´ë²¤íŠ¸ ì €ì¥ ì™„ë£Œ: {len(dividend_events_df)}í–‰")
        logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.dividend_events_table_path}")
    
    def save_data_to_parquet_zstd(self, df: pd.DataFrame, parquet_path: str, partition_cols: List[str] = None):
        """
        ë°ì´í„°ë¥¼ zstd ì••ì¶•ëœ Parquet íŒŒì¼ë¡œ ì €ì¥ (BigQuery ì§ì ‘ ì—°ë™ìš©)
        
        Args:
            df: ì €ì¥í•  DataFrame
            parquet_path: ì €ì¥ ê²½ë¡œ (gs://bucket/path)
            partition_cols: íŒŒí‹°ì…˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"\nğŸ’¾ ë°ì´í„°ë¥¼ zstd ì••ì¶• Parquetìœ¼ë¡œ ì €ì¥ ì¤‘...")
        
        if df.empty:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # pandas DataFrameì„ PyArrow Tableë¡œ ë³€í™˜
            table = pa.Table.from_pandas(df)
            
            if partition_cols:
                # íŒŒí‹°ì…˜ëœ Parquet ì €ì¥ (zstd ì••ì¶•)
                pq.write_to_dataset(
                    table,
                    root_path=parquet_path,
                    partition_cols=partition_cols,
                    compression="zstd",           # [ìˆ˜ì •] zstd ì••ì¶• ì ìš©
                    compression_level=5,          # [ìˆ˜ì •] ì••ì¶• ìˆ˜ì¤€ (1-22)
                    use_deprecated_int96_timestamps=False  # [ìˆ˜ì •] BigQuery í˜¸í™˜ì„±
                )
                logger.info(f"âœ… íŒŒí‹°ì…˜ëœ zstd Parquet ì €ì¥ ì™„ë£Œ: {len(df)}í–‰")
            else:
                # ë‹¨ì¼ Parquet íŒŒì¼ ì €ì¥ (zstd ì••ì¶•)
                pq.write_table(
                    table,
                    where=parquet_path,
                    compression="zstd",           # [ìˆ˜ì •] zstd ì••ì¶• ì ìš©
                    compression_level=5,          # [ìˆ˜ì •] ì••ì¶• ìˆ˜ì¤€
                    use_deprecated_int96_timestamps=False  # [ìˆ˜ì •] BigQuery í˜¸í™˜ì„±
                )
                logger.info(f"âœ… zstd Parquet ì €ì¥ ì™„ë£Œ: {len(df)}í–‰")
            
            logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {parquet_path}")
            
        except Exception as e:
            logger.error(f"âŒ zstd Parquet ì €ì¥ ì‹¤íŒ¨: {e}")
            raise Exception(f"zstd Parquet ì €ì¥ ì‹¤íŒ¨: {e}") from e
