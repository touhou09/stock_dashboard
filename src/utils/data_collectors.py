"""
Bronze Layer ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
- Wikipediaì—ì„œ S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
- yfinance APIë¥¼ í†µí•œ ê°€ê²© ë° ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘
"""

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta, timezone, date
import time
import random
import requests
from io import StringIO
from typing import List, Tuple
import logging

logger = logging.getLogger(__name__)

class SP500Collector:
    """S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ê¸° - ë‚ ì§œë³„ ì§€ì›"""
    
    WIKI_URL = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    
    def __init__(self):
        self.headers_pool = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/121.0",
        ]
        # ì—°ë„ë³„ S&P 500 ëª©ë¡ ìºì‹œ
        self._yearly_cache = {}
        self._current_sp500_df = None
    
    def to_yahoo_symbol(self, sym: str) -> str:
        """í´ë˜ìŠ¤ ì£¼ì‹ í‘œê¸°: BRK.B -> BRK-B"""
        return sym.strip().upper().replace(".", "-")
    
    def get_sp500_from_wikipedia(self, max_retries: int = 3, timeout: int = 15) -> pd.DataFrame:
        """Wikipediaì—ì„œ S&P500 êµ¬ì„±ì¢…ëª© í…Œì´ë¸” íŒŒì‹±"""
        last_err = None

        for i in range(max_retries):
            try:
                logger.info(f"Wikipedia ì ‘ê·¼ ì‹œë„ {i+1}/{max_retries}...")
                headers = {
                    "User-Agent": random.choice(self.headers_pool),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.7",
                    "Cache-Control": "no-cache",
                    "Pragma": "no-cache",
                }
                resp = requests.get(self.WIKI_URL, headers=headers, timeout=timeout)
                resp.raise_for_status()
                
                tables = pd.read_html(StringIO(resp.text))
                spx = tables[0]
                
                if "Symbol" not in spx.columns:
                    candidates = [c for c in spx.columns if "symbol" in c.lower() or "ticker" in c.lower()]
                    if candidates:
                        spx = spx.rename(columns={candidates[0]: "Symbol"})
                    else:
                        raise ValueError("Symbol ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                logger.info(f"âœ… Wikipediaì—ì„œ S&P 500 ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {len(spx)}ê°œ ì¢…ëª©")
                return spx
                
            except Exception as e:
                last_err = e
                logger.error(f"âŒ Wikipedia ì ‘ê·¼ ì‹¤íŒ¨ (ì‹œë„ {i+1}): {e}")
                if i < max_retries - 1:
                    wait_time = 1.5 * (i + 1)
                    logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
        
        raise RuntimeError(f"Wikipedia íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {last_err}")
    
    def normalize_symbols(self, df: pd.DataFrame) -> pd.DataFrame:
        """Yahoo í˜•ì‹ìœ¼ë¡œ ì‹¬ë³¼ ì •ê·œí™”"""
        df = df.copy()
        df["Symbol"] = df["Symbol"].astype(str).map(self.to_yahoo_symbol)
        return df
    
    def get_current_sp500_dataframe(self) -> pd.DataFrame:
        """í˜„ì¬ S&P 500 DataFrame ë°˜í™˜ (ìºì‹±)"""
        if self._current_sp500_df is None:
            logger.info("ğŸ“Š í˜„ì¬ S&P 500 ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            self._current_sp500_df = self.get_sp500_from_wikipedia()
            # í¸ì…ì¼ ì»¬ëŸ¼ ì¶”ê°€
            if 'Date added' in self._current_sp500_df.columns:
                self._current_sp500_df['Date added'] = pd.to_datetime(
                    self._current_sp500_df['Date added'], errors='coerce'
                )
        return self._current_sp500_df
    
    def get_sp500_tickers_for_date(self, target_date: datetime.date) -> List[str]:
        """
        íŠ¹ì • ë‚ ì§œì˜ S&P 500 êµ¬ì„± ì¢…ëª© ë°˜í™˜
        
        Args:
            target_date: ëŒ€ìƒ ë‚ ì§œ
            
        Returns:
            List[str]: í•´ë‹¹ ë‚ ì§œì˜ S&P 500 í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ“‹ {target_date} S&P 500 êµ¬ì„± ì¢…ëª© ì¡°íšŒ ì¤‘...")
        
        # í˜„ì¬ S&P 500 ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        sp500_df = self.get_current_sp500_dataframe()
        
        if 'Date added' not in sp500_df.columns:
            logger.warning("âš ï¸ í¸ì…ì¼ ì •ë³´ê°€ ì—†ì–´ì„œ í˜„ì¬ S&P 500 ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return sp500_df['Symbol'].dropna().unique().tolist()
        
        # í¸ì…ì¼ì´ ëŒ€ìƒ ë‚ ì§œ ì´ì „ì¸ ì¢…ëª©ë“¤ë§Œ í•„í„°ë§
        valid_tickers = sp500_df[
            sp500_df['Date added'] <= pd.Timestamp(target_date)
        ]['Symbol'].dropna().unique().tolist()
        
        # Yahoo Financeìš© ì‹¬ë³¼ë¡œ ë³€í™˜
        valid_tickers = [self.to_yahoo_symbol(ticker) for ticker in valid_tickers]
        
        logger.info(f"âœ… {target_date} S&P 500 êµ¬ì„± ì¢…ëª©: {len(valid_tickers)}ê°œ")
        return valid_tickers
    
    def get_sp500_tickers_for_year(self, year: int) -> List[str]:
        """
        íŠ¹ì • ì—°ë„ì˜ S&P 500 êµ¬ì„± ì¢…ëª© ë°˜í™˜ (ìºì‹±)
        
        Args:
            year: ëŒ€ìƒ ì—°ë„
            
        Returns:
            List[str]: í•´ë‹¹ ì—°ë„ì˜ S&P 500 í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        """
        if year not in self._yearly_cache:
            logger.info(f"ğŸ“‹ {year}ë…„ S&P 500 êµ¬ì„± ì¢…ëª© ìºì‹± ì¤‘...")
            
            # ì—°ë„ ë§ˆì§€ë§‰ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ
            year_end = date(year, 12, 31)
            self._yearly_cache[year] = self.get_sp500_tickers_for_date(year_end)
            
            logger.info(f"âœ… {year}ë…„ S&P 500 êµ¬ì„± ì¢…ëª© ìºì‹± ì™„ë£Œ: {len(self._yearly_cache[year])}ê°œ")
        
        return self._yearly_cache[year]
    
    def get_sp500_tickers_smart(self, start_date: datetime.date, end_date: datetime.date) -> List[str]:
        """
        ë°±í•„ ê¸°ê°„ì— ë§ëŠ” S&P 500 êµ¬ì„± ì¢…ëª© ë°˜í™˜ (ìŠ¤ë§ˆíŠ¸)
        
        Args:
            start_date: ë°±í•„ ì‹œì‘ ë‚ ì§œ
            end_date: ë°±í•„ ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            List[str]: ë°±í•„ì— ì í•©í•œ S&P 500 í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ“‹ ë°±í•„ ê¸°ê°„ {start_date} ~ {end_date} S&P 500 êµ¬ì„± ì¢…ëª© ì¡°íšŒ ì¤‘...")
        
        # ê°™ì€ ì—°ë„ì¸ ê²½ìš° ì—°ë„ë³„ ìºì‹± ì‚¬ìš©
        if start_date.year == end_date.year:
            return self.get_sp500_tickers_for_year(start_date.year)
        
        # ë‹¤ë¥¸ ì—°ë„ì¸ ê²½ìš° ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ
        return self.get_sp500_tickers_for_date(start_date)
    
    def get_historical_major_stocks_2000(self) -> List[str]:
        """
        2000ë…„ì— ìˆì—ˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ì£¼ìš” S&P 500 ì¢…ëª©ë“¤
        Wikipedia ë°ì´í„°ê°€ ë¶€ì •í™•í•œ ê²½ìš°ë¥¼ ìœ„í•œ ëŒ€ì•ˆ
        """
        return [
            # ê¸°ìˆ ì£¼
            'MSFT', 'IBM', 'INTC', 'CSCO', 'ORCL', 'SUNW', 'DELL', 'HPQ',
            
            # ê¸ˆìœµì£¼
            'JPM', 'BAC', 'C', 'WFC', 'GS', 'MS', 'AXP', 'USB', 'PNC', 'TFC',
            
            # ì œì¡°ì—…
            'GE', 'BA', 'CAT', 'MMM', 'HON', 'UTX', 'EMR', 'ITW', 'ETN', 'PH',
            
            # ì†Œë¹„ì¬
            'WMT', 'HD', 'PG', 'JNJ', 'KO', 'PEP', 'MCD', 'NKE', 'DIS', 'MCD',
            
            # ì—ë„ˆì§€
            'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'PXD', 'KMI', 'WMB', 'OKE', 'EPD',
            
            # í—¬ìŠ¤ì¼€ì–´
            'PFE', 'MRK', 'JNJ', 'ABT', 'TMO', 'DHR', 'BMY', 'LLY', 'AMGN', 'GILD',
            
            # í†µì‹ 
            'T', 'VZ', 'CMCSA', 'VZ', 'T', 'VZ', 'CMCSA', 'VZ', 'T', 'VZ',
            
            # ìœ í‹¸ë¦¬í‹°
            'SO', 'DUK', 'NEE', 'AEP', 'EXC', 'XEL', 'WEC', 'ES', 'PEG', 'ED',
            
            # ê¸°íƒ€
            'MCD', 'DIS', 'NKE', 'SBUX', 'COST', 'TGT', 'LOW', 'HD', 'WMT', 'PG'
        ]

class PriceDataCollector:
    """ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def get_daily_data_for_tickers(self, tickers: List[str], target_date: datetime.date) -> Tuple[List[pd.DataFrame], List[str], List[str]]:
        """ì „ì²´ S&P 500ì˜ í•˜ë£¨ì¹˜ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        logger.info(f"ğŸ“Š {target_date} í•˜ë£¨ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f"ğŸ“Š ì „ì²´ ì¢…ëª© ìˆ˜: {len(tickers)}ê°œ")
        
        all_daily_data = []
        successful_tickers = []
        failed_tickers = []
        
        for i, ticker in enumerate(tickers):
            logger.info(f"  ì²˜ë¦¬ ì¤‘: {ticker} ({i+1}/{len(tickers)})")
            
            try:
                stock = yf.Ticker(ticker)
                
                # í•˜ë£¨ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                start_date = target_date
                end_date = target_date + timedelta(days=1)
                
                hist = stock.history(start=start_date, end=end_date)
                
                if not hist.empty and hist['Close'].notna().any():
                    # ë°ì´í„° ì²˜ë¦¬ - Bronze ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ê·œí™”
                    hist['ticker'] = ticker
                    hist['date'] = hist.index.date  # ë‚ ì§œë§Œ ì¶”ì¶œ
                    hist = hist.reset_index(drop=True)
                    
                    # ì»¬ëŸ¼ëª… ì •ê·œí™” (Bronze ìŠ¤í‚¤ë§ˆ)
                    # [ìˆ˜ì •] Adj Closeê°€ ìˆì„ ë•Œë§Œ rename
                    rename_dict = {
                        'Open': 'open',
                        'High': 'high', 
                        'Low': 'low',
                        'Close': 'close',
                        'Volume': 'volume'
                    }
                    
                    if 'Adj Close' in hist.columns:
                        rename_dict['Adj Close'] = 'adj_close'
                    
                    hist = hist.rename(columns=rename_dict)
                    
                    # [ìˆ˜ì •] adj_closeê°€ ì—†ìœ¼ë©´ close ê°’ìœ¼ë¡œ ëŒ€ì²´
                    if 'adj_close' not in hist.columns:
                        hist['adj_close'] = hist['close']
                    
                    # ingest_at íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                    hist['ingest_at'] = datetime.now(timezone.utc)
                    
                    all_daily_data.append(hist)
                    successful_tickers.append(ticker)
                    
                    logger.info(f"    âœ… {ticker}: ${hist['close'].iloc[-1]:.2f}")
                else:
                    failed_tickers.append(ticker)
                    logger.info(f"    âŒ {ticker}: ë°ì´í„° ì—†ìŒ")
                    
            except Exception as e:
                failed_tickers.append(ticker)
                logger.error(f"    âŒ {ticker}: {e}")
            
            # API ì œí•œ ê³ ë ¤í•œ ë”œë ˆì´
            time.sleep(0.5)
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ (50ê°œë§ˆë‹¤)
            if (i + 1) % 50 == 0:
                logger.info(f"    ğŸ“Š ì§„í–‰ë¥ : {i+1}/{len(tickers)} ({((i+1)/len(tickers)*100):.1f}%)")
                logger.info(f"    âœ… ì„±ê³µ: {len(successful_tickers)}ê°œ, âŒ ì‹¤íŒ¨: {len(failed_tickers)}ê°œ")
        
        logger.info(f"\nğŸ“ˆ ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:")
        logger.info(f"  âœ… ì„±ê³µ: {len(successful_tickers)}ê°œ")
        logger.info(f"  âŒ ì‹¤íŒ¨: {len(failed_tickers)}ê°œ")
        logger.info(f"  ë°ì´í„° í¬ì¸íŠ¸: {sum(len(df) for df in all_daily_data)}ê°œ")
        
        return all_daily_data, successful_tickers, failed_tickers

class DividendDataCollector:
    """ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def fetch_dividend_events_for_tickers(self, tickers: List[str], since: datetime.date, until: datetime.date, collection_date: datetime.date = None) -> pd.DataFrame:
        """
        [Bronze] yfinance ë°°ë‹¹ ì´ë²¤íŠ¸ë¥¼ ì›ì²œ ê·¸ëŒ€ë¡œ ì ì¬ìš© DFë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ì…ë ¥: tickers, since(í¬í•¨), until(í¬í•¨)
        ì¶œë ¥: columns = [ex_date, ticker, amount, date, ingest_at]
        """
        logger.info(f"\nğŸ’° ë°°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì¤‘... ({since} ~ {until})")
        logger.info(f"ğŸ’° ì²˜ë¦¬í•  ì¢…ëª© ìˆ˜: {len(tickers)}ê°œ")
        
        # ìˆ˜ì§‘ì¼ ì„¤ì • (ê¸°ë³¸ê°’: ì˜¤ëŠ˜)
        if collection_date is None:
            collection_date = datetime.now().date()
        
        rows = []
        processed_count = 0
        
        for i, ticker in enumerate(tickers, 1):
            if i % 50 == 0 or i == 1:
                logger.info(f"  ğŸ“Š ì§„í–‰ë¥ : {i}/{len(tickers)} ({((i)/len(tickers)*100):.1f}%)")
            
            try:
                tk = yf.Ticker(ticker)
                divs = tk.dividends  # Series(index=ex-date, value=amount)
                
                if divs is None or divs.empty:
                    continue

                s = divs.copy()
                # ì¸ë±ìŠ¤ tz ì •ê·œí™”
                if hasattr(s.index, "tz"):
                    s.index = s.index.tz_convert("UTC").tz_localize(None)

                # ê¸°ê°„ í•„í„°
                mask = (s.index.date >= since) & (s.index.date <= until)
                s = s[mask]
                if s.empty:
                    continue

                for idx, amt in s.items():
                    rows.append({
                        "ex_date": idx.date(),
                        "ticker": ticker,
                        "amount": float(amt),
                        "date": collection_date,  # ìˆ˜ì§‘ì¼ ì¶”ê°€
                        "ingest_at": datetime.now(timezone.utc)
                    })
                    
            except Exception as e:
                logger.error(f"    âŒ {ticker}: {e}")
                continue
            
            # API ì œí•œ ê³ ë ¤
            time.sleep(0.3)
            processed_count += 1
        
        df = pd.DataFrame(rows)
        if not df.empty:
            df = df.sort_values(["ex_date", "ticker"]).reset_index(drop=True)
            logger.info(f"âœ… ë°°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ì´ë²¤íŠ¸")
        else:
            logger.info("âœ… ë°°ë‹¹ ì´ë²¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: 0ê°œ ì´ë²¤íŠ¸")
            
        return df
